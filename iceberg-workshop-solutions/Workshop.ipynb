{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "34577ad3-822f-4370-bcba-56b9fcec3196",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.sql._\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36mscala.sys.process._\u001b[39m"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql._\n",
    "import scala.sys.process._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6d3141ec-7779-467a-9f76-2e51030fd1c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "24/08/13 22:49:02 INFO SparkContext: Running Spark version 3.5.2\n",
      "24/08/13 22:49:02 INFO SparkContext: OS info Linux, 6.5.0-17-generic, amd64\n",
      "24/08/13 22:49:02 INFO SparkContext: Java version 11.0.24\n",
      "24/08/13 22:49:02 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "24/08/13 22:49:02 INFO ResourceUtils: ==============================================================\n",
      "24/08/13 22:49:02 INFO ResourceUtils: No custom resources configured for spark.driver.\n",
      "24/08/13 22:49:02 INFO ResourceUtils: ==============================================================\n",
      "24/08/13 22:49:02 INFO SparkContext: Submitted application: de374f24-5470-49f4-91e1-2f627c291cc4\n",
      "24/08/13 22:49:02 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)\n",
      "24/08/13 22:49:02 INFO ResourceProfile: Limiting resource is cpu\n",
      "24/08/13 22:49:02 INFO ResourceProfileManager: Added ResourceProfile id: 0\n",
      "24/08/13 22:49:02 INFO SecurityManager: Changing view acls to: dev\n",
      "24/08/13 22:49:02 INFO SecurityManager: Changing modify acls to: dev\n",
      "24/08/13 22:49:02 INFO SecurityManager: Changing view acls groups to: \n",
      "24/08/13 22:49:02 INFO SecurityManager: Changing modify acls groups to: \n",
      "24/08/13 22:49:02 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: dev; groups with view permissions: EMPTY; users with modify permissions: dev; groups with modify permissions: EMPTY\n",
      "24/08/13 22:49:03 INFO Utils: Successfully started service 'sparkDriver' on port 42409.\n",
      "24/08/13 22:49:03 INFO SparkEnv: Registering MapOutputTracker\n",
      "24/08/13 22:49:03 INFO SparkEnv: Registering BlockManagerMaster\n",
      "24/08/13 22:49:03 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information\n",
      "24/08/13 22:49:03 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up\n",
      "24/08/13 22:49:03 INFO SparkEnv: Registering BlockManagerMasterHeartbeat\n",
      "24/08/13 22:49:03 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-c05ed0fd-aba3-4f23-9a39-bff0e6847204\n",
      "24/08/13 22:49:03 INFO MemoryStore: MemoryStore started with capacity 13.8 GiB\n",
      "24/08/13 22:49:03 INFO SparkEnv: Registering OutputCommitCoordinator\n",
      "24/08/13 22:49:03 INFO JettyUtils: Start Jetty 0.0.0.0:4040 for SparkUI\n",
      "24/08/13 22:49:03 INFO Utils: Successfully started service 'SparkUI' on port 4040.\n",
      "24/08/13 22:49:03 INFO Executor: Starting executor ID driver on host fa3024105736\n",
      "24/08/13 22:49:03 INFO Executor: OS info Linux, 6.5.0-17-generic, amd64\n",
      "24/08/13 22:49:03 INFO Executor: Java version 11.0.24\n",
      "24/08/13 22:49:03 INFO Executor: Starting executor with user classpath (userClassPathFirst = false): ''\n",
      "24/08/13 22:49:03 INFO Executor: Created or updated repl class loader org.apache.spark.util.MutableURLClassLoader@23d3fa9b for default.\n",
      "24/08/13 22:49:03 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 40255.\n",
      "24/08/13 22:49:03 INFO NettyBlockTransferService: Server created on fa3024105736:40255\n",
      "24/08/13 22:49:03 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy\n",
      "24/08/13 22:49:03 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, fa3024105736, 40255, None)\n",
      "24/08/13 22:49:03 INFO BlockManagerMasterEndpoint: Registering block manager fa3024105736:40255 with 13.8 GiB RAM, BlockManagerId(driver, fa3024105736, 40255, None)\n",
      "24/08/13 22:49:03 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, fa3024105736, 40255, None)\n",
      "24/08/13 22:49:03 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, fa3024105736, 40255, None)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[36mspark\u001b[39m: \u001b[32mSparkSession\u001b[39m = org.apache.spark.sql.SparkSession@133f6af3\n",
       "\u001b[32mimport \u001b[39m\u001b[36mspark._\u001b[39m"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// So now we need to configure Spark to use Iceberg\n",
    "// See https://iceberg.apache.org/docs/1.6.0/spark-configuration/ & https://iceberg.apache.org/docs/1.6.0/spark-getting-started/\n",
    "// We'll use the \"hadoop\" (aka file) catalog & /high-performance-spark-examples/warehouse for the location\n",
    "val spark = (SparkSession.builder.master(\"local\")\n",
    "             // Setup the extensions\n",
    "             .config(\"spark.sql.extensions\", \"org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions\")\n",
    "             .config(\"spark.sql.catalog.local\", \"org.apache.iceberg.spark.SparkCatalog\")\n",
    "             .config(\"spark.sql.catalog.local.type\", \"hadoop\")\n",
    "             .config(\"spark.sql.catalog.local.warehouse\", \"/high-performance-spark-examples/warehouse\")\n",
    "             .getOrCreate()\n",
    "             )\n",
    "import spark._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "270730c9-9787-407c-ba22-f0cee1f67f53",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/08/13 22:49:03 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.\n",
      "24/08/13 22:49:03 INFO SharedState: Warehouse path is 'file:/high-performance-spark-examples/iceberg-workshop/spark-warehouse'.\n",
      "24/08/13 22:49:04 INFO InMemoryFileIndex: It took 24 ms to list leaf files for 1 paths.\n",
      "24/08/13 22:49:04 INFO InMemoryFileIndex: It took 1 ms to list leaf files for 1 paths.\n",
      "24/08/13 22:49:05 INFO FileSourceStrategy: Pushed Filters: \n",
      "24/08/13 22:49:05 INFO FileSourceStrategy: Post-Scan Filters: (length(trim(value#0, None)) > 0)\n",
      "24/08/13 22:49:05 INFO CodeGenerator: Code generated in 136.990334 ms\n",
      "24/08/13 22:49:05 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 199.5 KiB, free 13.8 GiB)\n",
      "24/08/13 22:49:05 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 34.5 KiB, free 13.8 GiB)\n",
      "24/08/13 22:49:05 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on fa3024105736:40255 (size: 34.5 KiB, free: 13.8 GiB)\n",
      "24/08/13 22:49:05 INFO SparkContext: Created broadcast 0 from csv at cmd2.sc:1\n",
      "24/08/13 22:49:05 INFO FileSourceScanExec: Planning scan with bin packing, max size: 8376950 bytes, open cost is considered as scanning 4194304 bytes.\n",
      "24/08/13 22:49:05 INFO SparkContext: Starting job: csv at cmd2.sc:1\n",
      "24/08/13 22:49:06 INFO DAGScheduler: Got job 0 (csv at cmd2.sc:1) with 1 output partitions\n",
      "24/08/13 22:49:06 INFO DAGScheduler: Final stage: ResultStage 0 (csv at cmd2.sc:1)\n",
      "24/08/13 22:49:06 INFO DAGScheduler: Parents of final stage: List()\n",
      "24/08/13 22:49:06 INFO DAGScheduler: Missing parents: List()\n",
      "24/08/13 22:49:06 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[3] at csv at cmd2.sc:1), which has no missing parents\n",
      "24/08/13 22:49:06 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 13.8 KiB, free 13.8 GiB)\n",
      "24/08/13 22:49:06 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 6.5 KiB, free 13.8 GiB)\n",
      "24/08/13 22:49:06 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on fa3024105736:40255 (size: 6.5 KiB, free: 13.8 GiB)\n",
      "24/08/13 22:49:06 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1585\n",
      "24/08/13 22:49:06 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[3] at csv at cmd2.sc:1) (first 15 tasks are for partitions Vector(0))\n",
      "24/08/13 22:49:06 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0\n",
      "24/08/13 22:49:06 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (fa3024105736, executor driver, partition 0, PROCESS_LOCAL, 9699 bytes) \n",
      "24/08/13 22:49:06 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)\n",
      "24/08/13 22:49:06 INFO CodeGenerator: Code generated in 8.455793 ms\n",
      "24/08/13 22:49:06 INFO FileScanRDD: Reading File path: file:///high-performance-spark-examples/data/fetched/2021, range: 0-4182646, partition values: [empty row]\n",
      "24/08/13 22:49:06 INFO CodeGenerator: Code generated in 9.872494 ms\n",
      "24/08/13 22:49:06 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 2002 bytes result sent to driver\n",
      "24/08/13 22:49:06 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 178 ms on fa3024105736 (executor driver) (1/1)\n",
      "24/08/13 22:49:06 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool \n",
      "24/08/13 22:49:06 INFO DAGScheduler: ResultStage 0 (csv at cmd2.sc:1) finished in 0.273 s\n",
      "24/08/13 22:49:06 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "24/08/13 22:49:06 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished\n",
      "24/08/13 22:49:06 INFO DAGScheduler: Job 0 finished: csv at cmd2.sc:1, took 0.305389 s\n",
      "24/08/13 22:49:06 INFO CodeGenerator: Code generated in 7.668273 ms\n",
      "24/08/13 22:49:06 INFO FileSourceStrategy: Pushed Filters: \n",
      "24/08/13 22:49:06 INFO FileSourceStrategy: Post-Scan Filters: \n",
      "24/08/13 22:49:06 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 199.5 KiB, free 13.8 GiB)\n",
      "24/08/13 22:49:06 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 34.5 KiB, free 13.8 GiB)\n",
      "24/08/13 22:49:06 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on fa3024105736:40255 (size: 34.5 KiB, free: 13.8 GiB)\n",
      "24/08/13 22:49:06 INFO SparkContext: Created broadcast 2 from csv at cmd2.sc:1\n",
      "24/08/13 22:49:06 INFO FileSourceScanExec: Planning scan with bin packing, max size: 8376950 bytes, open cost is considered as scanning 4194304 bytes.\n",
      "24/08/13 22:49:06 INFO SparkContext: Starting job: csv at cmd2.sc:1\n",
      "24/08/13 22:49:06 INFO DAGScheduler: Got job 1 (csv at cmd2.sc:1) with 1 output partitions\n",
      "24/08/13 22:49:06 INFO DAGScheduler: Final stage: ResultStage 1 (csv at cmd2.sc:1)\n",
      "24/08/13 22:49:06 INFO DAGScheduler: Parents of final stage: List()\n",
      "24/08/13 22:49:06 INFO DAGScheduler: Missing parents: List()\n",
      "24/08/13 22:49:06 INFO DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[9] at csv at cmd2.sc:1), which has no missing parents\n",
      "24/08/13 22:49:06 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 21.8 KiB, free 13.8 GiB)\n",
      "24/08/13 22:49:06 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 9.8 KiB, free 13.8 GiB)\n",
      "24/08/13 22:49:06 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on fa3024105736:40255 (size: 9.8 KiB, free: 13.8 GiB)\n",
      "24/08/13 22:49:06 INFO SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1585\n",
      "24/08/13 22:49:06 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[9] at csv at cmd2.sc:1) (first 15 tasks are for partitions Vector(0))\n",
      "24/08/13 22:49:06 INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks resource profile 0\n",
      "24/08/13 22:49:06 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (fa3024105736, executor driver, partition 0, PROCESS_LOCAL, 9699 bytes) \n",
      "24/08/13 22:49:06 INFO Executor: Running task 0.0 in stage 1.0 (TID 1)\n",
      "24/08/13 22:49:06 INFO CodeGenerator: Code generated in 5.475596 ms\n",
      "24/08/13 22:49:06 INFO FileScanRDD: Reading File path: file:///high-performance-spark-examples/data/fetched/2021, range: 0-4182646, partition values: [empty row]\n",
      "24/08/13 22:49:06 INFO Executor: Finished task 0.0 in stage 1.0 (TID 1). 1879 bytes result sent to driver\n",
      "24/08/13 22:49:06 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 153 ms on fa3024105736 (executor driver) (1/1)\n",
      "24/08/13 22:49:06 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool \n",
      "24/08/13 22:49:06 INFO DAGScheduler: ResultStage 1 (csv at cmd2.sc:1) finished in 0.172 s\n",
      "24/08/13 22:49:06 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "24/08/13 22:49:06 INFO TaskSchedulerImpl: Killing all running tasks in stage 1: Stage finished\n",
      "24/08/13 22:49:06 INFO DAGScheduler: Job 1 finished: csv at cmd2.sc:1, took 0.176415 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[36mdf\u001b[39m: \u001b[32mDataFrame\u001b[39m = [EmployerName: string, EmployerId: string ... 25 more fields]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Load the current data\n",
    "val df = spark.read.option(\"header\", \"true\").option(\"inferSchema\", \"true\").csv(\"/high-performance-spark-examples/data/fetched/2021\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "87ca6359-86bc-42a4-93dd-4fc64496b145",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/08/13 22:49:07 INFO CatalogUtil: Loading custom FileIO implementation: org.apache.iceberg.hadoop.HadoopFileIO\n",
      "24/08/13 22:49:07 INFO BaseMetastoreCatalog: Table loaded by catalog: local.uk_gender_pay_data\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[36mres3\u001b[39m: \u001b[32mDataFrame\u001b[39m = []"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Drop existing table if present & create new table\n",
    "spark.sql(\"DROP TABLE IF EXISTS local.uk_gender_pay_data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8bdeb3eb-b725-409b-ab3a-409d0e8309ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/08/13 22:49:07 WARN SparkStringUtils: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n",
      "24/08/13 22:49:07 INFO BaseMetastoreCatalog: Table properties set at catalog level through catalog properties: {}\n",
      "24/08/13 22:49:07 INFO BaseMetastoreCatalog: Table properties enforced at catalog level through catalog properties: {}\n",
      "24/08/13 22:49:07 INFO SparkWrite: Requesting 0 bytes advisory partition size for table uk_gender_pay_data\n",
      "24/08/13 22:49:07 INFO SparkWrite: Requesting UnspecifiedDistribution as write distribution for table uk_gender_pay_data\n",
      "24/08/13 22:49:07 INFO SparkWrite: Requesting [] as write ordering for table uk_gender_pay_data\n",
      "24/08/13 22:49:07 INFO FileSourceStrategy: Pushed Filters: \n",
      "24/08/13 22:49:07 INFO FileSourceStrategy: Post-Scan Filters: \n",
      "24/08/13 22:49:07 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 199.4 KiB, free 13.8 GiB)\n",
      "24/08/13 22:49:07 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 34.4 KiB, free 13.8 GiB)\n",
      "24/08/13 22:49:07 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on fa3024105736:40255 (size: 34.4 KiB, free: 13.8 GiB)\n",
      "24/08/13 22:49:07 INFO SparkContext: Created broadcast 4 from saveAsTable at cmd4.sc:1\n",
      "24/08/13 22:49:07 INFO FileSourceScanExec: Planning scan with bin packing, max size: 8376950 bytes, open cost is considered as scanning 4194304 bytes.\n",
      "24/08/13 22:49:07 INFO MemoryStore: Block broadcast_5 stored as values in memory (estimated size 32.0 KiB, free 13.8 GiB)\n",
      "24/08/13 22:49:07 INFO MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 29.1 KiB, free 13.8 GiB)\n",
      "24/08/13 22:49:07 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on fa3024105736:40255 (size: 29.1 KiB, free: 13.8 GiB)\n",
      "24/08/13 22:49:07 INFO SparkContext: Created broadcast 5 from broadcast at SparkWrite.java:193\n",
      "24/08/13 22:49:07 INFO AppendDataExec: Start processing data source write support: IcebergBatchWrite(table=uk_gender_pay_data, format=PARQUET). The input RDD has 1 partitions.\n",
      "24/08/13 22:49:07 INFO SparkContext: Starting job: saveAsTable at cmd4.sc:1\n",
      "24/08/13 22:49:07 INFO DAGScheduler: Got job 2 (saveAsTable at cmd4.sc:1) with 1 output partitions\n",
      "24/08/13 22:49:07 INFO DAGScheduler: Final stage: ResultStage 2 (saveAsTable at cmd4.sc:1)\n",
      "24/08/13 22:49:07 INFO DAGScheduler: Parents of final stage: List()\n",
      "24/08/13 22:49:07 INFO DAGScheduler: Missing parents: List()\n",
      "24/08/13 22:49:07 INFO DAGScheduler: Submitting ResultStage 2 (MapPartitionsRDD[11] at saveAsTable at cmd4.sc:1), which has no missing parents\n",
      "24/08/13 22:49:07 INFO MemoryStore: Block broadcast_6 stored as values in memory (estimated size 17.9 KiB, free 13.8 GiB)\n",
      "24/08/13 22:49:07 INFO MemoryStore: Block broadcast_6_piece0 stored as bytes in memory (estimated size 8.5 KiB, free 13.8 GiB)\n",
      "24/08/13 22:49:07 INFO BlockManagerInfo: Added broadcast_6_piece0 in memory on fa3024105736:40255 (size: 8.5 KiB, free: 13.8 GiB)\n",
      "24/08/13 22:49:07 INFO SparkContext: Created broadcast 6 from broadcast at DAGScheduler.scala:1585\n",
      "24/08/13 22:49:07 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 2 (MapPartitionsRDD[11] at saveAsTable at cmd4.sc:1) (first 15 tasks are for partitions Vector(0))\n",
      "24/08/13 22:49:07 INFO TaskSchedulerImpl: Adding task set 2.0 with 1 tasks resource profile 0\n",
      "24/08/13 22:49:07 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 2) (fa3024105736, executor driver, partition 0, PROCESS_LOCAL, 9699 bytes) \n",
      "24/08/13 22:49:07 INFO Executor: Running task 0.0 in stage 2.0 (TID 2)\n",
      "24/08/13 22:49:07 INFO CodecPool: Got brand-new compressor [.zstd]\n",
      "24/08/13 22:49:07 INFO FileScanRDD: Reading File path: file:///high-performance-spark-examples/data/fetched/2021, range: 0-4182646, partition values: [empty row]\n",
      "24/08/13 22:49:07 INFO CodeGenerator: Code generated in 25.033432 ms\n",
      "24/08/13 22:49:08 INFO BlockManagerInfo: Removed broadcast_0_piece0 on fa3024105736:40255 in memory (size: 34.5 KiB, free: 13.8 GiB)\n",
      "24/08/13 22:49:08 INFO BlockManagerInfo: Removed broadcast_1_piece0 on fa3024105736:40255 in memory (size: 6.5 KiB, free: 13.8 GiB)\n",
      "24/08/13 22:49:08 INFO BlockManagerInfo: Removed broadcast_2_piece0 on fa3024105736:40255 in memory (size: 34.5 KiB, free: 13.8 GiB)\n",
      "24/08/13 22:49:08 INFO BlockManagerInfo: Removed broadcast_3_piece0 on fa3024105736:40255 in memory (size: 9.8 KiB, free: 13.8 GiB)\n",
      "24/08/13 22:49:08 INFO DataWritingSparkTask: Writer for partition 0 is committing.\n",
      "24/08/13 22:49:08 INFO DataWritingSparkTask: Committed partition 0 (task 2, attempt 0, stage 2.0)\n",
      "24/08/13 22:49:08 INFO Executor: Finished task 0.0 in stage 2.0 (TID 2). 6910 bytes result sent to driver\n",
      "24/08/13 22:49:08 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 2) in 918 ms on fa3024105736 (executor driver) (1/1)\n",
      "24/08/13 22:49:08 INFO TaskSchedulerImpl: Removed TaskSet 2.0, whose tasks have all completed, from pool \n",
      "24/08/13 22:49:08 INFO DAGScheduler: ResultStage 2 (saveAsTable at cmd4.sc:1) finished in 0.927 s\n",
      "24/08/13 22:49:08 INFO DAGScheduler: Job 2 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "24/08/13 22:49:08 INFO TaskSchedulerImpl: Killing all running tasks in stage 2: Stage finished\n",
      "24/08/13 22:49:08 INFO DAGScheduler: Job 2 finished: saveAsTable at cmd4.sc:1, took 0.930188 s\n",
      "24/08/13 22:49:08 INFO AppendDataExec: Data source write support IcebergBatchWrite(table=uk_gender_pay_data, format=PARQUET) is committing.\n",
      "24/08/13 22:49:08 INFO SparkWrite: Committing append with 1 new data files to table uk_gender_pay_data\n",
      "24/08/13 22:49:08 INFO SnapshotProducer: Committed snapshot 3775120664731591 (MergeAppend)\n",
      "24/08/13 22:49:08 INFO LoggingMetricsReporter: Received metrics report: CommitReport{tableName=uk_gender_pay_data, snapshotId=3775120664731591, sequenceNumber=1, operation=append, commitMetrics=CommitMetricsResult{totalDuration=TimerResult{timeUnit=NANOSECONDS, totalDuration=PT0.215443936S, count=1}, attempts=CounterResult{unit=COUNT, value=1}, addedDataFiles=CounterResult{unit=COUNT, value=1}, removedDataFiles=null, totalDataFiles=CounterResult{unit=COUNT, value=1}, addedDeleteFiles=null, addedEqualityDeleteFiles=null, addedPositionalDeleteFiles=null, removedDeleteFiles=null, removedEqualityDeleteFiles=null, removedPositionalDeleteFiles=null, totalDeleteFiles=CounterResult{unit=COUNT, value=0}, addedRecords=CounterResult{unit=COUNT, value=15395}, removedRecords=null, totalRecords=CounterResult{unit=COUNT, value=15395}, addedFilesSizeInBytes=CounterResult{unit=BYTES, value=1294005}, removedFilesSizeInBytes=null, totalFilesSizeInBytes=CounterResult{unit=BYTES, value=1294005}, addedPositionalDeletes=null, removedPositionalDeletes=null, totalPositionalDeletes=CounterResult{unit=COUNT, value=0}, addedEqualityDeletes=null, removedEqualityDeletes=null, totalEqualityDeletes=CounterResult{unit=COUNT, value=0}}, metadata={engine-version=3.5.2, app-id=local-1723589343398, engine-name=spark, iceberg-version=Apache Iceberg 1.6.0 (commit 229d8f6fcd109e6c8943ea7cbb41dab746c6d0ed)}}\n",
      "24/08/13 22:49:08 INFO SparkWrite: Committed in 226 ms\n",
      "24/08/13 22:49:08 INFO AppendDataExec: Data source write support IcebergBatchWrite(table=uk_gender_pay_data, format=PARQUET) committed.\n",
      "24/08/13 22:49:08 INFO HadoopTableOperations: Committed a new metadata file /high-performance-spark-examples/warehouse/uk_gender_pay_data/metadata/v1.metadata.json\n"
     ]
    }
   ],
   "source": [
    "// Write the data out\n",
    "df.write.saveAsTable(\"local.uk_gender_pay_data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "554c6036-0c6b-4e3c-a9e1-7251c608b48f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mres5\u001b[39m: \u001b[32mString\u001b[39m = \u001b[32m\"\"\"c8c28535-2d82-40b1-a36d-fcfc8885f1be-m0.avro\n",
       "snap-3775120664731591-1-c8c28535-2d82-40b1-a36d-fcfc8885f1be.avro\n",
       "v1.metadata.json\n",
       "version-hint.text\n",
       "\"\"\"\u001b[39m"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"ls /high-performance-spark-examples/warehouse/uk_gender_pay_data/metadata/\".!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cb541fbf-4a79-402d-a6b2-e999106e9a18",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mres6\u001b[39m: \u001b[32mString\u001b[39m = \u001b[32m\"\"\"{\n",
       "  \"format-version\" : 2,\n",
       "  \"table-uuid\" : \"f464c7c6-9962-453c-9705-aa243b3e7f55\",\n",
       "  \"location\" : \"/high-performance-spark-examples/warehouse/uk_gender_pay_data\",\n",
       "  \"last-sequence-number\" : 1,\n",
       "  \"last-updated-ms\" : 1723589348730,\n",
       "  \"last-column-id\" : 27,\n",
       "  \"current-schema-id\" : 0,\n",
       "  \"schemas\" : [ {\n",
       "    \"type\" : \"struct\",\n",
       "    \"schema-id\" : 0,\n",
       "    \"fields\" : [ {\n",
       "      \"id\" : 1,\n",
       "      \"name\" : \"EmployerName\",\n",
       "      \"required\" : false,\n",
       "      \"type\" : \"string\"\n",
       "    }, {\n",
       "      \"id\" : 2,\n",
       "      \"name\" : \"EmployerId\",\n",
       "      \"required\" : false,\n",
       "      \"type\" : \"string\"\n",
       "    }, {\n",
       "      \"id\" : 3,\n",
       "      \"name\" : \"Address\",\n",
       "      \"required\" : false,\n",
       "      \"type\" : \"string\"\n",
       "    }, {\n",
       "      \"id\" : 4,\n",
       "      \"name\" : \"PostCode\",\n",
       "      \"required\" : false,\n",
       "      \"type\" : \"string\"\n",
       "    }, {\n",
       "      \"id\" : 5,\n",
       "      \"name\" : \"CompanyNumber\",\n",
       "      \"required\" : false,\n",
       "      \"type\" : \"string\"\n",
       "    }, {\n",
       "      \"id\" : 6,\n",
       "      \"name\" : \"SicCodes\",\n",
       "\u001b[39m..."
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"cat  /high-performance-spark-examples/warehouse/uk_gender_pay_data/metadata/v1.metadata.json\".!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "90149834-27a2-45a3-aa8a-dae2162da854",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[39m\u001b[36mjava.util.HashMap\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36mjava.util.Map\n",
       "\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.iceberg.Table\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.iceberg.catalog.TableIdentifier\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.iceberg.hadoop.HadoopCatalog\n",
       "\n",
       "// And to handle java types\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36mscala.jdk.CollectionConverters._\u001b[39m"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Java SDK time imports\n",
    "import java.util.HashMap\n",
    "import java.util.Map\n",
    "\n",
    "import org.apache.iceberg.Table\n",
    "import org.apache.iceberg.catalog.TableIdentifier\n",
    "import org.apache.iceberg.hadoop.HadoopCatalog\n",
    "\n",
    "// And to handle java types\n",
    "import scala.jdk.CollectionConverters._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ebf56bc6-d420-474c-b3a8-ded03b23eff8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/08/13 22:49:09 INFO CatalogUtil: Loading custom FileIO implementation: org.apache.iceberg.hadoop.HadoopFileIO\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[36mcatalog\u001b[39m: \u001b[32mHadoopCatalog\u001b[39m = HadoopCatalog{name=hadoop, location=/high-performance-spark-examples/warehouse}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val catalog = new HadoopCatalog(spark.sparkContext.hadoopConfiguration, \"/high-performance-spark-examples/warehouse\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c55dc276-035f-40d4-9a47-bd4698f2519d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mname\u001b[39m: \u001b[32mTableIdentifier\u001b[39m = uk_gender_pay_data"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val name = TableIdentifier.of(\"uk_gender_pay_data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8ea4b1cc-bd1b-42b4-bdbe-27625b461db9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/08/13 22:49:09 INFO BaseMetastoreCatalog: Table loaded by catalog: hadoop.uk_gender_pay_data\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[36mtable\u001b[39m: \u001b[32mTable\u001b[39m = hadoop.uk_gender_pay_data"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val table = catalog.loadTable(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dd1c6add-d465-4b81-9c34-6c8f40197ab2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36msnapshots\u001b[39m: \u001b[32mList\u001b[39m[\u001b[32morg\u001b[39m.\u001b[32mapache\u001b[39m.\u001b[32miceberg\u001b[39m.\u001b[32mSnapshot\u001b[39m] = \u001b[33mList\u001b[39m(\n",
       "  BaseSnapshot{id=3775120664731591, timestamp_ms=1723589348730, operation=append, summary={spark.app.id=local-1723589343398, added-data-files=1, added-records=15395, added-files-size=1294005, changed-partition-count=1, total-records=15395, total-files-size=1294005, total-data-files=1, total-delete-files=0, total-position-deletes=0, total-equality-deletes=0, engine-version=3.5.2, app-id=local-1723589343398, engine-name=spark, iceberg-version=Apache Iceberg 1.6.0 (commit 229d8f6fcd109e6c8943ea7cbb41dab746c6d0ed)}, manifest-list=/high-performance-spark-examples/warehouse/uk_gender_pay_data/metadata/snap-3775120664731591-1-c8c28535-2d82-40b1-a36d-fcfc8885f1be.avro, schema-id=0}\n",
       ")\n",
       "\u001b[36mres11_1\u001b[39m: \u001b[32mList\u001b[39m[\u001b[32morg\u001b[39m.\u001b[32mapache\u001b[39m.\u001b[32miceberg\u001b[39m.\u001b[32mSnapshot\u001b[39m] = \u001b[33mList\u001b[39m(\n",
       "  BaseSnapshot{id=3775120664731591, timestamp_ms=1723589348730, operation=append, summary={spark.app.id=local-1723589343398, added-data-files=1, added-records=15395, added-files-size=1294005, changed-partition-count=1, total-records=15395, total-files-size=1294005, total-data-files=1, total-delete-files=0, total-position-deletes=0, total-equality-deletes=0, engine-version=3.5.2, app-id=local-1723589343398, engine-name=spark, iceberg-version=Apache Iceberg 1.6.0 (commit 229d8f6fcd109e6c8943ea7cbb41dab746c6d0ed)}, manifest-list=/high-performance-spark-examples/warehouse/uk_gender_pay_data/metadata/snap-3775120664731591-1-c8c28535-2d82-40b1-a36d-fcfc8885f1be.avro, schema-id=0}\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val snapshots = table.snapshots().asScala.toList\n",
    "snapshots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9a96986d-b3a5-49ad-aeac-a492bf3fc8e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36msnapshot\u001b[39m: \u001b[32mLong\u001b[39m = \u001b[32m3775120664731591L\u001b[39m"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val snapshot = snapshots(0).snapshotId()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "15c6cb85-ff64-405f-ae6a-7e3c917ac12a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/08/13 22:51:31 INFO BaseMetastoreCatalog: Table loaded by catalog: local.uk_gender_pay_data\n",
      "24/08/13 22:51:31 INFO V2ScanRelationPushDown: \n",
      "Output: committed_at#377, snapshot_id#378L, parent_id#379L, operation#380, manifest_list#381, summary#382\n",
      "         \n",
      "24/08/13 22:51:31 INFO SparkPartitioningAwareScan: Reporting UnknownPartitioning with 1 partition(s) for table local.uk_gender_pay_data.snapshots\n",
      "24/08/13 22:51:31 INFO MemoryStore: Block broadcast_13 stored as values in memory (estimated size 32.0 KiB, free 13.8 GiB)\n",
      "24/08/13 22:51:31 INFO MemoryStore: Block broadcast_13_piece0 stored as bytes in memory (estimated size 29.2 KiB, free 13.8 GiB)\n",
      "24/08/13 22:51:31 INFO BlockManagerInfo: Added broadcast_13_piece0 in memory on fa3024105736:40255 (size: 29.2 KiB, free: 13.8 GiB)\n",
      "24/08/13 22:51:31 INFO SparkContext: Created broadcast 13 from broadcast at SparkBatch.java:79\n",
      "24/08/13 22:51:31 INFO MemoryStore: Block broadcast_14 stored as values in memory (estimated size 32.0 KiB, free 13.8 GiB)\n",
      "24/08/13 22:51:31 INFO MemoryStore: Block broadcast_14_piece0 stored as bytes in memory (estimated size 29.2 KiB, free 13.8 GiB)\n",
      "24/08/13 22:51:31 INFO BlockManagerInfo: Added broadcast_14_piece0 in memory on fa3024105736:40255 (size: 29.2 KiB, free: 13.8 GiB)\n",
      "24/08/13 22:51:31 INFO SparkContext: Created broadcast 14 from broadcast at SparkBatch.java:79\n",
      "24/08/13 22:51:31 INFO CodeGenerator: Code generated in 12.801144 ms\n",
      "24/08/13 22:51:31 INFO SparkContext: Starting job: show at cmd15.sc:2\n",
      "24/08/13 22:51:31 INFO DAGScheduler: Got job 5 (show at cmd15.sc:2) with 1 output partitions\n",
      "24/08/13 22:51:31 INFO DAGScheduler: Final stage: ResultStage 5 (show at cmd15.sc:2)\n",
      "24/08/13 22:51:31 INFO DAGScheduler: Parents of final stage: List()\n",
      "24/08/13 22:51:31 INFO DAGScheduler: Missing parents: List()\n",
      "24/08/13 22:51:31 INFO DAGScheduler: Submitting ResultStage 5 (MapPartitionsRDD[23] at show at cmd15.sc:2), which has no missing parents\n",
      "24/08/13 22:51:31 INFO MemoryStore: Block broadcast_15 stored as values in memory (estimated size 15.7 KiB, free 13.8 GiB)\n",
      "24/08/13 22:51:31 INFO MemoryStore: Block broadcast_15_piece0 stored as bytes in memory (estimated size 6.2 KiB, free 13.8 GiB)\n",
      "24/08/13 22:51:31 INFO BlockManagerInfo: Added broadcast_15_piece0 in memory on fa3024105736:40255 (size: 6.2 KiB, free: 13.8 GiB)\n",
      "24/08/13 22:51:31 INFO SparkContext: Created broadcast 15 from broadcast at DAGScheduler.scala:1585\n",
      "24/08/13 22:51:31 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 5 (MapPartitionsRDD[23] at show at cmd15.sc:2) (first 15 tasks are for partitions Vector(0))\n",
      "24/08/13 22:51:31 INFO TaskSchedulerImpl: Adding task set 5.0 with 1 tasks resource profile 0\n",
      "24/08/13 22:51:31 INFO TaskSetManager: Starting task 0.0 in stage 5.0 (TID 5) (fa3024105736, executor driver, partition 0, PROCESS_LOCAL, 14155 bytes) \n",
      "24/08/13 22:51:31 INFO Executor: Running task 0.0 in stage 5.0 (TID 5)\n",
      "24/08/13 22:51:31 INFO CodeGenerator: Code generated in 14.996517 ms\n",
      "24/08/13 22:51:31 INFO Executor: Finished task 0.0 in stage 5.0 (TID 5). 4865 bytes result sent to driver\n",
      "24/08/13 22:51:31 INFO TaskSetManager: Finished task 0.0 in stage 5.0 (TID 5) in 32 ms on fa3024105736 (executor driver) (1/1)\n",
      "24/08/13 22:51:31 INFO TaskSchedulerImpl: Removed TaskSet 5.0, whose tasks have all completed, from pool \n",
      "24/08/13 22:51:31 INFO DAGScheduler: ResultStage 5 (show at cmd15.sc:2) finished in 0.046 s\n",
      "24/08/13 22:51:31 INFO DAGScheduler: Job 5 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "24/08/13 22:51:31 INFO TaskSchedulerImpl: Killing all running tasks in stage 5: Stage finished\n",
      "24/08/13 22:51:31 INFO DAGScheduler: Job 5 finished: show at cmd15.sc:2, took 0.052238 s\n",
      "24/08/13 22:51:31 INFO CodeGenerator: Code generated in 9.643032 ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----------------+---------+---------+--------------------+--------------------+\n",
      "|        committed_at|     snapshot_id|parent_id|operation|       manifest_list|             summary|\n",
      "+--------------------+----------------+---------+---------+--------------------+--------------------+\n",
      "|2024-08-13 22:49:...|3775120664731591|     NULL|   append|/high-performance...|{spark.app.id -> ...|\n",
      "+--------------------+----------------+---------+---------+--------------------+--------------------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[36maltSnapshotQuery\u001b[39m: \u001b[32mDataFrame\u001b[39m = [committed_at: timestamp, snapshot_id: bigint ... 4 more fields]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val altSnapshotQuery = spark.sql(\"SELECT * FROM local.uk_gender_pay_data.snapshots\")\n",
    "altSnapshotQuery.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f93516ad-3ae9-4bb6-989f-7c127f82143c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/08/13 22:52:01 INFO BaseMetastoreCatalog: Table loaded by catalog: local.uk_gender_pay_data\n",
      "24/08/13 22:52:01 INFO V2ScanRelationPushDown: \n",
      "Output: snapshot_id#426L\n",
      "         \n",
      "24/08/13 22:52:01 INFO SparkPartitioningAwareScan: Reporting UnknownPartitioning with 1 partition(s) for table local.uk_gender_pay_data.snapshots\n",
      "24/08/13 22:52:01 INFO MemoryStore: Block broadcast_16 stored as values in memory (estimated size 32.0 KiB, free 13.8 GiB)\n",
      "24/08/13 22:52:02 INFO MemoryStore: Block broadcast_16_piece0 stored as bytes in memory (estimated size 29.2 KiB, free 13.8 GiB)\n",
      "24/08/13 22:52:02 INFO BlockManagerInfo: Added broadcast_16_piece0 in memory on fa3024105736:40255 (size: 29.2 KiB, free: 13.8 GiB)\n",
      "24/08/13 22:52:02 INFO SparkContext: Created broadcast 16 from broadcast at SparkBatch.java:79\n",
      "24/08/13 22:52:02 INFO MemoryStore: Block broadcast_17 stored as values in memory (estimated size 32.0 KiB, free 13.8 GiB)\n",
      "24/08/13 22:52:02 INFO MemoryStore: Block broadcast_17_piece0 stored as bytes in memory (estimated size 29.2 KiB, free 13.8 GiB)\n",
      "24/08/13 22:52:02 INFO BlockManagerInfo: Added broadcast_17_piece0 in memory on fa3024105736:40255 (size: 29.2 KiB, free: 13.8 GiB)\n",
      "24/08/13 22:52:02 INFO SparkContext: Created broadcast 17 from broadcast at SparkBatch.java:79\n",
      "24/08/13 22:52:02 INFO CodeGenerator: Code generated in 7.48024 ms\n",
      "24/08/13 22:52:02 INFO SparkContext: Starting job: collect at cmd16.sc:1\n",
      "24/08/13 22:52:02 INFO DAGScheduler: Got job 6 (collect at cmd16.sc:1) with 1 output partitions\n",
      "24/08/13 22:52:02 INFO DAGScheduler: Final stage: ResultStage 6 (collect at cmd16.sc:1)\n",
      "24/08/13 22:52:02 INFO DAGScheduler: Parents of final stage: List()\n",
      "24/08/13 22:52:02 INFO DAGScheduler: Missing parents: List()\n",
      "24/08/13 22:52:02 INFO DAGScheduler: Submitting ResultStage 6 (MapPartitionsRDD[27] at collect at cmd16.sc:1), which has no missing parents\n",
      "24/08/13 22:52:02 INFO MemoryStore: Block broadcast_18 stored as values in memory (estimated size 11.8 KiB, free 13.8 GiB)\n",
      "24/08/13 22:52:02 INFO MemoryStore: Block broadcast_18_piece0 stored as bytes in memory (estimated size 4.9 KiB, free 13.8 GiB)\n",
      "24/08/13 22:52:02 INFO BlockManagerInfo: Added broadcast_18_piece0 in memory on fa3024105736:40255 (size: 4.9 KiB, free: 13.8 GiB)\n",
      "24/08/13 22:52:02 INFO SparkContext: Created broadcast 18 from broadcast at DAGScheduler.scala:1585\n",
      "24/08/13 22:52:02 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 6 (MapPartitionsRDD[27] at collect at cmd16.sc:1) (first 15 tasks are for partitions Vector(0))\n",
      "24/08/13 22:52:02 INFO TaskSchedulerImpl: Adding task set 6.0 with 1 tasks resource profile 0\n",
      "24/08/13 22:52:02 INFO TaskSetManager: Starting task 0.0 in stage 6.0 (TID 6) (fa3024105736, executor driver, partition 0, PROCESS_LOCAL, 13734 bytes) \n",
      "24/08/13 22:52:02 INFO Executor: Running task 0.0 in stage 6.0 (TID 6)\n",
      "24/08/13 22:52:02 INFO CodeGenerator: Code generated in 7.799029 ms\n",
      "24/08/13 22:52:02 INFO Executor: Finished task 0.0 in stage 6.0 (TID 6). 4364 bytes result sent to driver\n",
      "24/08/13 22:52:02 INFO TaskSetManager: Finished task 0.0 in stage 6.0 (TID 6) in 23 ms on fa3024105736 (executor driver) (1/1)\n",
      "24/08/13 22:52:02 INFO TaskSchedulerImpl: Removed TaskSet 6.0, whose tasks have all completed, from pool \n",
      "24/08/13 22:52:02 INFO DAGScheduler: ResultStage 6 (collect at cmd16.sc:1) finished in 0.034 s\n",
      "24/08/13 22:52:02 INFO DAGScheduler: Job 6 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "24/08/13 22:52:02 INFO TaskSchedulerImpl: Killing all running tasks in stage 6: Stage finished\n",
      "24/08/13 22:52:02 INFO DAGScheduler: Job 6 finished: collect at cmd16.sc:1, took 0.038209 s\n",
      "24/08/13 22:52:02 INFO CodeGenerator: Code generated in 8.015035 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[36maltSnapshotId\u001b[39m: \u001b[32mRow\u001b[39m = [3775120664731591]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val altSnapshotId = spark.sql(\"SELECT snapshot_id FROM local.uk_gender_pay_data.snapshots\").collect()(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "15e67d1b-1e9e-45a0-af94-1c9c79e03d54",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/08/13 22:52:13 INFO V2ScanRelationPushDown: \n",
      "Pushing operators to local.uk_gender_pay_data\n",
      "Pushed Filters: ResponsiblePerson IS NULL\n",
      "Post-Scan Filters: isnull(ResponsiblePerson#461)\n",
      "         \n",
      "24/08/13 22:52:13 INFO V2ScanRelationPushDown: \n",
      "Output: EmployerName#440, EmployerId#441, Address#442, PostCode#443, CompanyNumber#444, SicCodes#445, DiffMeanHourlyPercent#446, DiffMedianHourlyPercent#447, DiffMeanBonusPercent#448, DiffMedianBonusPercent#449, MaleBonusPercent#450, FemaleBonusPercent#451, MaleLowerQuartile#452, FemaleLowerQuartile#453, MaleLowerMiddleQuartile#454, FemaleLowerMiddleQuartile#455, MaleUpperMiddleQuartile#456, FemaleUpperMiddleQuartile#457, MaleTopQuartile#458, FemaleTopQuartile#459, CompanyLinkToGPGInfo#460, ResponsiblePerson#461, EmployerSize#462, CurrentName#463, SubmittedAfterTheDeadline#464, DueDate#465, DateSubmitted#466\n",
      "         \n",
      "24/08/13 22:52:13 INFO SnapshotScan: Scanning table local.uk_gender_pay_data snapshot 3775120664731591 created at 2024-08-13T22:49:08.730+00:00 with filter ResponsiblePerson IS NULL\n",
      "24/08/13 22:52:13 INFO BaseDistributedDataScan: Planning file tasks locally for table local.uk_gender_pay_data\n",
      "24/08/13 22:52:13 INFO SparkPartitioningAwareScan: Reporting UnknownPartitioning with 1 partition(s) for table local.uk_gender_pay_data\n",
      "24/08/13 22:52:13 INFO MemoryStore: Block broadcast_19 stored as values in memory (estimated size 32.0 KiB, free 13.8 GiB)\n",
      "24/08/13 22:52:13 INFO MemoryStore: Block broadcast_19_piece0 stored as bytes in memory (estimated size 29.4 KiB, free 13.8 GiB)\n",
      "24/08/13 22:52:13 INFO BlockManagerInfo: Added broadcast_19_piece0 in memory on fa3024105736:40255 (size: 29.4 KiB, free: 13.8 GiB)\n",
      "24/08/13 22:52:13 INFO SparkContext: Created broadcast 19 from broadcast at SparkBatch.java:79\n",
      "24/08/13 22:52:13 INFO MemoryStore: Block broadcast_20 stored as values in memory (estimated size 32.0 KiB, free 13.8 GiB)\n",
      "24/08/13 22:52:13 INFO MemoryStore: Block broadcast_20_piece0 stored as bytes in memory (estimated size 29.3 KiB, free 13.8 GiB)\n",
      "24/08/13 22:52:13 INFO BlockManagerInfo: Added broadcast_20_piece0 in memory on fa3024105736:40255 (size: 29.3 KiB, free: 13.8 GiB)\n",
      "24/08/13 22:52:13 INFO SparkContext: Created broadcast 20 from broadcast at SparkBatch.java:79\n",
      "24/08/13 22:52:13 INFO SparkContext: Starting job: show at cmd17.sc:1\n",
      "24/08/13 22:52:13 INFO DAGScheduler: Got job 7 (show at cmd17.sc:1) with 1 output partitions\n",
      "24/08/13 22:52:13 INFO DAGScheduler: Final stage: ResultStage 7 (show at cmd17.sc:1)\n",
      "24/08/13 22:52:13 INFO DAGScheduler: Parents of final stage: List()\n",
      "24/08/13 22:52:13 INFO DAGScheduler: Missing parents: List()\n",
      "24/08/13 22:52:13 INFO DAGScheduler: Submitting ResultStage 7 (MapPartitionsRDD[31] at show at cmd17.sc:1), which has no missing parents\n",
      "24/08/13 22:52:13 INFO MemoryStore: Block broadcast_21 stored as values in memory (estimated size 30.2 KiB, free 13.8 GiB)\n",
      "24/08/13 22:52:13 INFO MemoryStore: Block broadcast_21_piece0 stored as bytes in memory (estimated size 7.9 KiB, free 13.8 GiB)\n",
      "24/08/13 22:52:13 INFO BlockManagerInfo: Added broadcast_21_piece0 in memory on fa3024105736:40255 (size: 7.9 KiB, free: 13.8 GiB)\n",
      "24/08/13 22:52:13 INFO SparkContext: Created broadcast 21 from broadcast at DAGScheduler.scala:1585\n",
      "24/08/13 22:52:13 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 7 (MapPartitionsRDD[31] at show at cmd17.sc:1) (first 15 tasks are for partitions Vector(0))\n",
      "24/08/13 22:52:13 INFO TaskSchedulerImpl: Adding task set 7.0 with 1 tasks resource profile 0\n",
      "24/08/13 22:52:13 INFO TaskSetManager: Starting task 0.0 in stage 7.0 (TID 7) (fa3024105736, executor driver, partition 0, PROCESS_LOCAL, 17271 bytes) \n",
      "24/08/13 22:52:13 INFO Executor: Running task 0.0 in stage 7.0 (TID 7)\n",
      "24/08/13 22:52:13 INFO CodecPool: Got brand-new decompressor [.zstd]\n",
      "24/08/13 22:52:13 INFO Executor: Finished task 0.0 in stage 7.0 (TID 7). 5709 bytes result sent to driver\n",
      "24/08/13 22:52:13 INFO TaskSetManager: Finished task 0.0 in stage 7.0 (TID 7) in 60 ms on fa3024105736 (executor driver) (1/1)\n",
      "24/08/13 22:52:13 INFO TaskSchedulerImpl: Removed TaskSet 7.0, whose tasks have all completed, from pool \n",
      "24/08/13 22:52:13 INFO DAGScheduler: ResultStage 7 (show at cmd17.sc:1) finished in 0.079 s\n",
      "24/08/13 22:52:13 INFO DAGScheduler: Job 7 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "24/08/13 22:52:13 INFO TaskSchedulerImpl: Killing all running tasks in stage 7: Stage finished\n",
      "24/08/13 22:52:13 INFO DAGScheduler: Job 7 finished: show at cmd17.sc:1, took 0.106005 s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----------+--------------------+--------------------+-------------+--------+---------------------+-----------------------+--------------------+----------------------+----------------+------------------+-----------------+-------------------+-----------------------+-------------------------+-----------------------+-------------------------+---------------+-----------------+--------------------+-----------------+--------------------+-----------+-------------------------+---------+-------------+\n",
      "|        EmployerName|EmployerId|             Address|            PostCode|CompanyNumber|SicCodes|DiffMeanHourlyPercent|DiffMedianHourlyPercent|DiffMeanBonusPercent|DiffMedianBonusPercent|MaleBonusPercent|FemaleBonusPercent|MaleLowerQuartile|FemaleLowerQuartile|MaleLowerMiddleQuartile|FemaleLowerMiddleQuartile|MaleUpperMiddleQuartile|FemaleUpperMiddleQuartile|MaleTopQuartile|FemaleTopQuartile|CompanyLinkToGPGInfo|ResponsiblePerson|        EmployerSize|CurrentName|SubmittedAfterTheDeadline|  DueDate|DateSubmitted|\n",
      "+--------------------+----------+--------------------+--------------------+-------------+--------+---------------------+-----------------------+--------------------+----------------------+----------------+------------------+-----------------+-------------------+-----------------------+-------------------------+-----------------------+-------------------------+---------------+-----------------+--------------------+-----------------+--------------------+-----------+-------------------------+---------+-------------+\n",
      "|\"\"\"RED BAND\"\" CHE...|  LIMITED\"|               16879|19 Smith's Place,...|      EH6 8NU|SC016876|                47730|                    0.6|                -4.4|                  14.1|            -2.0|              21.8|             78.2|               41.0|                   59.0|                     23.2|                   76.8|                      4.9|           95.1|             20.5|                79.5|             NULL|Philip Galt (Mana...| 250 to 499|     \"\"\"RED BAND\"\" CHE...| LIMITED\"|        False|\n",
      "|          1509 GROUP|     15320|Royal Grammar Sch...|             GU1 3BB|     04104101|  85200,|                 NULL|                   NULL|                NULL|                  NULL|            NULL|              NULL|             NULL|               NULL|                   NULL|                     NULL|                   NULL|                     NULL|           NULL|             NULL|                NULL|             NULL|                NULL|       NULL|                     NULL|     NULL|         NULL|\n",
      "|1LIFE MANAGEMENT ...|       687|The Stables, Duxb...|             PR7 4AT|     02566586|  93110,|                 NULL|                   NULL|                NULL|                  NULL|            NULL|              NULL|             NULL|               NULL|                   NULL|                     NULL|                   NULL|                     NULL|           NULL|             NULL|                NULL|             NULL|                NULL|       NULL|                     NULL|     NULL|         NULL|\n",
      "|               93130|      NULL|                NULL|                NULL|         NULL|    NULL|                 NULL|                   NULL|                NULL|                  NULL|            NULL|              NULL|             NULL|               NULL|                   NULL|                     NULL|                   NULL|                     NULL|           NULL|             NULL|                NULL|             NULL|                NULL|       NULL|                     NULL|     NULL|         NULL|\n",
      "|  1ST HOME CARE LTD.|     17484|Suite 1, Ground F...|             G32 9AT|     SC272838|  86900,|                 NULL|                   NULL|                NULL|                  NULL|            NULL|              NULL|             NULL|               NULL|                   NULL|                     NULL|                   NULL|                     NULL|           NULL|             NULL|                NULL|             NULL|                NULL|       NULL|                     NULL|     NULL|         NULL|\n",
      "+--------------------+----------+--------------------+--------------------+-------------+--------+---------------------+-----------------------+--------------------+----------------------+----------------+------------------+-----------------+-------------------+-----------------------+-------------------------+-----------------------+-------------------------+---------------+-----------------+--------------------+-----------------+--------------------+-----------+-------------------------+---------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT * FROM local.uk_gender_pay_data WHERE isnull(responsibleperson) LIMIT 5\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d94eb4db-bf03-49be-865a-e80c0613d526",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/08/13 22:52:14 INFO V2ScanRelationPushDown: \n",
      "Output: committed_at#635, snapshot_id#636L, parent_id#637L, operation#638, manifest_list#639, summary#640\n",
      "         \n",
      "24/08/13 22:52:14 INFO SparkPartitioningAwareScan: Reporting UnknownPartitioning with 1 partition(s) for table local.uk_gender_pay_data.snapshots\n",
      "24/08/13 22:52:14 INFO MemoryStore: Block broadcast_22 stored as values in memory (estimated size 32.0 KiB, free 13.8 GiB)\n",
      "24/08/13 22:52:14 INFO MemoryStore: Block broadcast_22_piece0 stored as bytes in memory (estimated size 29.2 KiB, free 13.8 GiB)\n",
      "24/08/13 22:52:14 INFO BlockManagerInfo: Added broadcast_22_piece0 in memory on fa3024105736:40255 (size: 29.2 KiB, free: 13.8 GiB)\n",
      "24/08/13 22:52:14 INFO SparkContext: Created broadcast 22 from broadcast at SparkBatch.java:79\n",
      "24/08/13 22:52:14 INFO MemoryStore: Block broadcast_23 stored as values in memory (estimated size 32.0 KiB, free 13.8 GiB)\n",
      "24/08/13 22:52:14 INFO MemoryStore: Block broadcast_23_piece0 stored as bytes in memory (estimated size 29.2 KiB, free 13.8 GiB)\n",
      "24/08/13 22:52:14 INFO BlockManagerInfo: Added broadcast_23_piece0 in memory on fa3024105736:40255 (size: 29.2 KiB, free: 13.8 GiB)\n",
      "24/08/13 22:52:14 INFO SparkContext: Created broadcast 23 from broadcast at SparkBatch.java:79\n",
      "24/08/13 22:52:14 INFO SparkContext: Starting job: show at cmd18.sc:1\n",
      "24/08/13 22:52:14 INFO DAGScheduler: Got job 8 (show at cmd18.sc:1) with 1 output partitions\n",
      "24/08/13 22:52:14 INFO DAGScheduler: Final stage: ResultStage 8 (show at cmd18.sc:1)\n",
      "24/08/13 22:52:14 INFO DAGScheduler: Parents of final stage: List()\n",
      "24/08/13 22:52:14 INFO DAGScheduler: Missing parents: List()\n",
      "24/08/13 22:52:14 INFO DAGScheduler: Submitting ResultStage 8 (MapPartitionsRDD[35] at show at cmd18.sc:1), which has no missing parents\n",
      "24/08/13 22:52:14 INFO MemoryStore: Block broadcast_24 stored as values in memory (estimated size 15.7 KiB, free 13.8 GiB)\n",
      "24/08/13 22:52:14 INFO MemoryStore: Block broadcast_24_piece0 stored as bytes in memory (estimated size 6.2 KiB, free 13.8 GiB)\n",
      "24/08/13 22:52:14 INFO BlockManagerInfo: Added broadcast_24_piece0 in memory on fa3024105736:40255 (size: 6.2 KiB, free: 13.8 GiB)\n",
      "24/08/13 22:52:14 INFO SparkContext: Created broadcast 24 from broadcast at DAGScheduler.scala:1585\n",
      "24/08/13 22:52:14 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 8 (MapPartitionsRDD[35] at show at cmd18.sc:1) (first 15 tasks are for partitions Vector(0))\n",
      "24/08/13 22:52:14 INFO TaskSchedulerImpl: Adding task set 8.0 with 1 tasks resource profile 0\n",
      "24/08/13 22:52:14 INFO TaskSetManager: Starting task 0.0 in stage 8.0 (TID 8) (fa3024105736, executor driver, partition 0, PROCESS_LOCAL, 14155 bytes) \n",
      "24/08/13 22:52:14 INFO Executor: Running task 0.0 in stage 8.0 (TID 8)\n",
      "24/08/13 22:52:14 INFO Executor: Finished task 0.0 in stage 8.0 (TID 8). 4865 bytes result sent to driver\n",
      "24/08/13 22:52:14 INFO TaskSetManager: Finished task 0.0 in stage 8.0 (TID 8) in 16 ms on fa3024105736 (executor driver) (1/1)\n",
      "24/08/13 22:52:14 INFO TaskSchedulerImpl: Removed TaskSet 8.0, whose tasks have all completed, from pool \n",
      "24/08/13 22:52:14 INFO DAGScheduler: ResultStage 8 (show at cmd18.sc:1) finished in 0.028 s\n",
      "24/08/13 22:52:14 INFO DAGScheduler: Job 8 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "24/08/13 22:52:14 INFO TaskSchedulerImpl: Killing all running tasks in stage 8: Stage finished\n",
      "24/08/13 22:52:14 INFO DAGScheduler: Job 8 finished: show at cmd18.sc:1, took 0.032053 s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----------------+---------+---------+--------------------+--------------------+\n",
      "|        committed_at|     snapshot_id|parent_id|operation|       manifest_list|             summary|\n",
      "+--------------------+----------------+---------+---------+--------------------+--------------------+\n",
      "|2024-08-13 22:49:...|3775120664731591|     NULL|   append|/high-performance...|{spark.app.id -> ...|\n",
      "+--------------------+----------------+---------+---------+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "// We can also list snapshots with the select\n",
    "spark.sql(\"SELECT * FROM local.uk_gender_pay_data.snapshots\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4829752b-dc30-49db-93ae-911f1c2743c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/08/13 22:52:15 INFO V2ScanRelationPushDown: \n",
      "Output: content#683, file_path#684, file_format#685, spec_id#686, record_count#687L, file_size_in_bytes#688L, column_sizes#689, value_counts#690, null_value_counts#691, nan_value_counts#692, lower_bounds#693, upper_bounds#694, key_metadata#695, split_offsets#696, equality_ids#697, sort_order_id#698, readable_metrics#699\n",
      "         \n",
      "24/08/13 22:52:15 INFO SnapshotScan: Scanning table local.uk_gender_pay_data snapshot 3775120664731591 created at 2024-08-13T22:49:08.730+00:00 with filter true\n",
      "24/08/13 22:52:15 INFO SparkPartitioningAwareScan: Reporting UnknownPartitioning with 1 partition(s) for table local.uk_gender_pay_data.files\n",
      "24/08/13 22:52:15 INFO MemoryStore: Block broadcast_25 stored as values in memory (estimated size 32.0 KiB, free 13.8 GiB)\n",
      "24/08/13 22:52:15 INFO MemoryStore: Block broadcast_25_piece0 stored as bytes in memory (estimated size 32.4 KiB, free 13.8 GiB)\n",
      "24/08/13 22:52:15 INFO BlockManagerInfo: Added broadcast_25_piece0 in memory on fa3024105736:40255 (size: 32.4 KiB, free: 13.8 GiB)\n",
      "24/08/13 22:52:15 INFO SparkContext: Created broadcast 25 from broadcast at SparkBatch.java:79\n",
      "24/08/13 22:52:15 INFO MemoryStore: Block broadcast_26 stored as values in memory (estimated size 32.0 KiB, free 13.8 GiB)\n",
      "24/08/13 22:52:15 INFO MemoryStore: Block broadcast_26_piece0 stored as bytes in memory (estimated size 32.4 KiB, free 13.8 GiB)\n",
      "24/08/13 22:52:15 INFO BlockManagerInfo: Added broadcast_26_piece0 in memory on fa3024105736:40255 (size: 32.4 KiB, free: 13.8 GiB)\n",
      "24/08/13 22:52:15 INFO SparkContext: Created broadcast 26 from broadcast at SparkBatch.java:79\n",
      "24/08/13 22:52:15 INFO SparkContext: Starting job: show at cmd19.sc:1\n",
      "24/08/13 22:52:15 INFO DAGScheduler: Got job 9 (show at cmd19.sc:1) with 1 output partitions\n",
      "24/08/13 22:52:15 INFO DAGScheduler: Final stage: ResultStage 9 (show at cmd19.sc:1)\n",
      "24/08/13 22:52:15 INFO DAGScheduler: Parents of final stage: List()\n",
      "24/08/13 22:52:15 INFO DAGScheduler: Missing parents: List()\n",
      "24/08/13 22:52:15 INFO DAGScheduler: Submitting ResultStage 9 (MapPartitionsRDD[39] at show at cmd19.sc:1), which has no missing parents\n",
      "24/08/13 22:52:15 INFO MemoryStore: Block broadcast_27 stored as values in memory (estimated size 43.0 KiB, free 13.8 GiB)\n",
      "24/08/13 22:52:15 INFO MemoryStore: Block broadcast_27_piece0 stored as bytes in memory (estimated size 10.7 KiB, free 13.8 GiB)\n",
      "24/08/13 22:52:15 INFO BlockManagerInfo: Added broadcast_27_piece0 in memory on fa3024105736:40255 (size: 10.7 KiB, free: 13.8 GiB)\n",
      "24/08/13 22:52:15 INFO SparkContext: Created broadcast 27 from broadcast at DAGScheduler.scala:1585\n",
      "24/08/13 22:52:15 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 9 (MapPartitionsRDD[39] at show at cmd19.sc:1) (first 15 tasks are for partitions Vector(0))\n",
      "24/08/13 22:52:15 INFO TaskSchedulerImpl: Adding task set 9.0 with 1 tasks resource profile 0\n",
      "24/08/13 22:52:15 INFO TaskSetManager: Starting task 0.0 in stage 9.0 (TID 9) (fa3024105736, executor driver, partition 0, PROCESS_LOCAL, 124815 bytes) \n",
      "24/08/13 22:52:15 INFO Executor: Running task 0.0 in stage 9.0 (TID 9)\n",
      "24/08/13 22:52:15 INFO CodeGenerator: Code generated in 119.736939 ms\n",
      "24/08/13 22:52:15 INFO Executor: Finished task 0.0 in stage 9.0 (TID 9). 7203 bytes result sent to driver\n",
      "24/08/13 22:52:15 INFO TaskSetManager: Finished task 0.0 in stage 9.0 (TID 9) in 246 ms on fa3024105736 (executor driver) (1/1)\n",
      "24/08/13 22:52:15 INFO TaskSchedulerImpl: Removed TaskSet 9.0, whose tasks have all completed, from pool \n",
      "24/08/13 22:52:15 INFO DAGScheduler: ResultStage 9 (show at cmd19.sc:1) finished in 0.285 s\n",
      "24/08/13 22:52:15 INFO DAGScheduler: Job 9 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "24/08/13 22:52:15 INFO TaskSchedulerImpl: Killing all running tasks in stage 9: Stage finished\n",
      "24/08/13 22:52:15 INFO DAGScheduler: Job 9 finished: show at cmd19.sc:1, took 0.292114 s\n",
      "24/08/13 22:52:16 INFO CodeGenerator: Code generated in 15.58866 ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------------+-----------+-------+------------+------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+------------+-------------+------------+-------------+--------------------+\n",
      "|content|           file_path|file_format|spec_id|record_count|file_size_in_bytes|        column_sizes|        value_counts|   null_value_counts|    nan_value_counts|        lower_bounds|        upper_bounds|key_metadata|split_offsets|equality_ids|sort_order_id|    readable_metrics|\n",
      "+-------+--------------------+-----------+-------+------------+------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+------------+-------------+------------+-------------+--------------------+\n",
      "|      0|/high-performance...|    PARQUET|      0|       15395|           1294005|{1 -> 139005, 2 -...|{1 -> 15395, 2 ->...|{1 -> 0, 2 -> 157...|{8 -> 0, 9 -> 0, ...|{1 -> [22 22 22 5...|{1 -> [77 61 72 6...|        NULL|          [4]|        NULL|            0|{{238188, 15395, ...|\n",
      "+-------+--------------------+-----------+-------+------------+------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+------------+-------------+------------+-------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "// And the files!\n",
    "// We can also list snapshots with the select\n",
    "spark.sql(\"SELECT * FROM local.uk_gender_pay_data.files\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f262d890-0818-410a-aec8-2986a04ae16e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/08/13 22:52:16 INFO V2ScanRelationPushDown: \n",
      "Pushing operators to local.uk_gender_pay_data\n",
      "Pushed Filters: ResponsiblePerson IS NULL\n",
      "Post-Scan Filters: isnull(ResponsiblePerson#829)\n",
      "         \n",
      "24/08/13 22:52:16 INFO V2ScanRelationPushDown: \n",
      "Output: EmployerName#808, EmployerId#809, Address#810, PostCode#811, CompanyNumber#812, SicCodes#813, DiffMeanHourlyPercent#814, DiffMedianHourlyPercent#815, DiffMeanBonusPercent#816, DiffMedianBonusPercent#817, MaleBonusPercent#818, FemaleBonusPercent#819, MaleLowerQuartile#820, FemaleLowerQuartile#821, MaleLowerMiddleQuartile#822, FemaleLowerMiddleQuartile#823, MaleUpperMiddleQuartile#824, FemaleUpperMiddleQuartile#825, MaleTopQuartile#826, FemaleTopQuartile#827, CompanyLinkToGPGInfo#828, ResponsiblePerson#829, EmployerSize#830, CurrentName#831, SubmittedAfterTheDeadline#832, DueDate#833, DateSubmitted#834\n",
      "         \n",
      "24/08/13 22:52:16 INFO SnapshotScan: Scanning table local.uk_gender_pay_data snapshot 3775120664731591 created at 2024-08-13T22:49:08.730+00:00 with filter ResponsiblePerson IS NULL\n",
      "24/08/13 22:52:16 INFO BaseDistributedDataScan: Planning file tasks locally for table local.uk_gender_pay_data\n",
      "24/08/13 22:52:16 INFO SparkPartitioningAwareScan: Reporting UnknownPartitioning with 1 partition(s) for table local.uk_gender_pay_data\n",
      "24/08/13 22:52:16 INFO MemoryStore: Block broadcast_28 stored as values in memory (estimated size 32.0 KiB, free 13.8 GiB)\n",
      "24/08/13 22:52:16 INFO MemoryStore: Block broadcast_28_piece0 stored as bytes in memory (estimated size 29.4 KiB, free 13.8 GiB)\n",
      "24/08/13 22:52:16 INFO BlockManagerInfo: Added broadcast_28_piece0 in memory on fa3024105736:40255 (size: 29.4 KiB, free: 13.8 GiB)\n",
      "24/08/13 22:52:16 INFO SparkContext: Created broadcast 28 from broadcast at SparkBatch.java:79\n",
      "24/08/13 22:52:16 INFO MemoryStore: Block broadcast_29 stored as values in memory (estimated size 32.0 KiB, free 13.8 GiB)\n",
      "24/08/13 22:52:16 INFO MemoryStore: Block broadcast_29_piece0 stored as bytes in memory (estimated size 29.3 KiB, free 13.8 GiB)\n",
      "24/08/13 22:52:16 INFO BlockManagerInfo: Added broadcast_29_piece0 in memory on fa3024105736:40255 (size: 29.3 KiB, free: 13.8 GiB)\n",
      "24/08/13 22:52:16 INFO SparkContext: Created broadcast 29 from broadcast at SparkBatch.java:79\n",
      "24/08/13 22:52:16 INFO SparkContext: Starting job: show at cmd20.sc:1\n",
      "24/08/13 22:52:16 INFO DAGScheduler: Got job 10 (show at cmd20.sc:1) with 1 output partitions\n",
      "24/08/13 22:52:16 INFO DAGScheduler: Final stage: ResultStage 10 (show at cmd20.sc:1)\n",
      "24/08/13 22:52:16 INFO DAGScheduler: Parents of final stage: List()\n",
      "24/08/13 22:52:16 INFO DAGScheduler: Missing parents: List()\n",
      "24/08/13 22:52:16 INFO DAGScheduler: Submitting ResultStage 10 (MapPartitionsRDD[43] at show at cmd20.sc:1), which has no missing parents\n",
      "24/08/13 22:52:16 INFO MemoryStore: Block broadcast_30 stored as values in memory (estimated size 30.2 KiB, free 13.8 GiB)\n",
      "24/08/13 22:52:16 INFO MemoryStore: Block broadcast_30_piece0 stored as bytes in memory (estimated size 7.9 KiB, free 13.8 GiB)\n",
      "24/08/13 22:52:16 INFO BlockManagerInfo: Added broadcast_30_piece0 in memory on fa3024105736:40255 (size: 7.9 KiB, free: 13.8 GiB)\n",
      "24/08/13 22:52:16 INFO SparkContext: Created broadcast 30 from broadcast at DAGScheduler.scala:1585\n",
      "24/08/13 22:52:16 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 10 (MapPartitionsRDD[43] at show at cmd20.sc:1) (first 15 tasks are for partitions Vector(0))\n",
      "24/08/13 22:52:16 INFO TaskSchedulerImpl: Adding task set 10.0 with 1 tasks resource profile 0\n",
      "24/08/13 22:52:16 INFO TaskSetManager: Starting task 0.0 in stage 10.0 (TID 10) (fa3024105736, executor driver, partition 0, PROCESS_LOCAL, 17271 bytes) \n",
      "24/08/13 22:52:16 INFO Executor: Running task 0.0 in stage 10.0 (TID 10)\n",
      "24/08/13 22:52:16 INFO CodecPool: Got brand-new decompressor [.zstd]\n",
      "24/08/13 22:52:16 INFO Executor: Finished task 0.0 in stage 10.0 (TID 10). 5709 bytes result sent to driver\n",
      "24/08/13 22:52:16 INFO TaskSetManager: Finished task 0.0 in stage 10.0 (TID 10) in 40 ms on fa3024105736 (executor driver) (1/1)\n",
      "24/08/13 22:52:16 INFO TaskSchedulerImpl: Removed TaskSet 10.0, whose tasks have all completed, from pool \n",
      "24/08/13 22:52:16 INFO DAGScheduler: ResultStage 10 (show at cmd20.sc:1) finished in 0.048 s\n",
      "24/08/13 22:52:16 INFO DAGScheduler: Job 10 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "24/08/13 22:52:16 INFO TaskSchedulerImpl: Killing all running tasks in stage 10: Stage finished\n",
      "24/08/13 22:52:16 INFO DAGScheduler: Job 10 finished: show at cmd20.sc:1, took 0.053955 s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----------+--------------------+--------------------+-------------+--------+---------------------+-----------------------+--------------------+----------------------+----------------+------------------+-----------------+-------------------+-----------------------+-------------------------+-----------------------+-------------------------+---------------+-----------------+--------------------+-----------------+--------------------+-----------+-------------------------+---------+-------------+\n",
      "|        EmployerName|EmployerId|             Address|            PostCode|CompanyNumber|SicCodes|DiffMeanHourlyPercent|DiffMedianHourlyPercent|DiffMeanBonusPercent|DiffMedianBonusPercent|MaleBonusPercent|FemaleBonusPercent|MaleLowerQuartile|FemaleLowerQuartile|MaleLowerMiddleQuartile|FemaleLowerMiddleQuartile|MaleUpperMiddleQuartile|FemaleUpperMiddleQuartile|MaleTopQuartile|FemaleTopQuartile|CompanyLinkToGPGInfo|ResponsiblePerson|        EmployerSize|CurrentName|SubmittedAfterTheDeadline|  DueDate|DateSubmitted|\n",
      "+--------------------+----------+--------------------+--------------------+-------------+--------+---------------------+-----------------------+--------------------+----------------------+----------------+------------------+-----------------+-------------------+-----------------------+-------------------------+-----------------------+-------------------------+---------------+-----------------+--------------------+-----------------+--------------------+-----------+-------------------------+---------+-------------+\n",
      "|\"\"\"RED BAND\"\" CHE...|  LIMITED\"|               16879|19 Smith's Place,...|      EH6 8NU|SC016876|                47730|                    0.6|                -4.4|                  14.1|            -2.0|              21.8|             78.2|               41.0|                   59.0|                     23.2|                   76.8|                      4.9|           95.1|             20.5|                79.5|             NULL|Philip Galt (Mana...| 250 to 499|     \"\"\"RED BAND\"\" CHE...| LIMITED\"|        False|\n",
      "|          1509 GROUP|     15320|Royal Grammar Sch...|             GU1 3BB|     04104101|  85200,|                 NULL|                   NULL|                NULL|                  NULL|            NULL|              NULL|             NULL|               NULL|                   NULL|                     NULL|                   NULL|                     NULL|           NULL|             NULL|                NULL|             NULL|                NULL|       NULL|                     NULL|     NULL|         NULL|\n",
      "|1LIFE MANAGEMENT ...|       687|The Stables, Duxb...|             PR7 4AT|     02566586|  93110,|                 NULL|                   NULL|                NULL|                  NULL|            NULL|              NULL|             NULL|               NULL|                   NULL|                     NULL|                   NULL|                     NULL|           NULL|             NULL|                NULL|             NULL|                NULL|       NULL|                     NULL|     NULL|         NULL|\n",
      "|               93130|      NULL|                NULL|                NULL|         NULL|    NULL|                 NULL|                   NULL|                NULL|                  NULL|            NULL|              NULL|             NULL|               NULL|                   NULL|                     NULL|                   NULL|                     NULL|           NULL|             NULL|                NULL|             NULL|                NULL|       NULL|                     NULL|     NULL|         NULL|\n",
      "|  1ST HOME CARE LTD.|     17484|Suite 1, Ground F...|             G32 9AT|     SC272838|  86900,|                 NULL|                   NULL|                NULL|                  NULL|            NULL|              NULL|             NULL|               NULL|                   NULL|                     NULL|                   NULL|                     NULL|           NULL|             NULL|                NULL|             NULL|                NULL|       NULL|                     NULL|     NULL|         NULL|\n",
      "+--------------------+----------+--------------------+--------------------+-------------+--------+---------------------+-----------------------+--------------------+----------------------+----------------+------------------+-----------------+-------------------+-----------------------+-------------------------+-----------------------+-------------------------+---------------+-----------------+--------------------+-----------------+--------------------+-----------+-------------------------+---------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "// Lets take a quick look and see\n",
    "spark.sql(\"SELECT * FROM local.uk_gender_pay_data WHERE isnull(responsibleperson) LIMIT 5\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7369bcc8-a738-48dc-a475-55885d4460cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/08/13 22:52:16 INFO BlockManagerInfo: Removed broadcast_7_piece0 on fa3024105736:40255 in memory (size: 29.4 KiB, free: 13.8 GiB)\n",
      "24/08/13 22:52:16 INFO BlockManagerInfo: Removed broadcast_8_piece0 on fa3024105736:40255 in memory (size: 29.3 KiB, free: 13.8 GiB)\n",
      "24/08/13 22:52:16 INFO BlockManagerInfo: Removed broadcast_13_piece0 on fa3024105736:40255 in memory (size: 29.2 KiB, free: 13.8 GiB)\n",
      "24/08/13 22:52:16 INFO BlockManagerInfo: Removed broadcast_23_piece0 on fa3024105736:40255 in memory (size: 29.2 KiB, free: 13.8 GiB)\n",
      "24/08/13 22:52:16 INFO BlockManagerInfo: Removed broadcast_6_piece0 on fa3024105736:40255 in memory (size: 8.5 KiB, free: 13.8 GiB)\n",
      "24/08/13 22:52:16 INFO BlockManagerInfo: Removed broadcast_16_piece0 on fa3024105736:40255 in memory (size: 29.2 KiB, free: 13.8 GiB)\n",
      "24/08/13 22:52:16 INFO BlockManagerInfo: Removed broadcast_18_piece0 on fa3024105736:40255 in memory (size: 4.9 KiB, free: 13.8 GiB)\n",
      "24/08/13 22:52:16 INFO BlockManagerInfo: Removed broadcast_15_piece0 on fa3024105736:40255 in memory (size: 6.2 KiB, free: 13.8 GiB)\n",
      "24/08/13 22:52:16 INFO BlockManagerInfo: Removed broadcast_25_piece0 on fa3024105736:40255 in memory (size: 32.4 KiB, free: 13.8 GiB)\n",
      "24/08/13 22:52:16 INFO BlockManagerInfo: Removed broadcast_17_piece0 on fa3024105736:40255 in memory (size: 29.2 KiB, free: 13.8 GiB)\n",
      "24/08/13 22:52:16 INFO BlockManagerInfo: Removed broadcast_14_piece0 on fa3024105736:40255 in memory (size: 29.2 KiB, free: 13.8 GiB)\n",
      "24/08/13 22:52:16 INFO BlockManagerInfo: Removed broadcast_24_piece0 on fa3024105736:40255 in memory (size: 6.2 KiB, free: 13.8 GiB)\n",
      "24/08/13 22:52:16 INFO BlockManagerInfo: Removed broadcast_26_piece0 on fa3024105736:40255 in memory (size: 32.4 KiB, free: 13.8 GiB)\n",
      "24/08/13 22:52:16 INFO BlockManagerInfo: Removed broadcast_12_piece0 on fa3024105736:40255 in memory (size: 6.1 KiB, free: 13.8 GiB)\n",
      "24/08/13 22:52:16 INFO BlockManagerInfo: Removed broadcast_22_piece0 on fa3024105736:40255 in memory (size: 29.2 KiB, free: 13.8 GiB)\n",
      "24/08/13 22:52:16 INFO BlockManagerInfo: Removed broadcast_27_piece0 on fa3024105736:40255 in memory (size: 10.7 KiB, free: 13.8 GiB)\n",
      "24/08/13 22:52:16 INFO BlockManagerInfo: Removed broadcast_21_piece0 on fa3024105736:40255 in memory (size: 7.9 KiB, free: 13.8 GiB)\n",
      "24/08/13 22:52:16 INFO BlockManagerInfo: Removed broadcast_10_piece0 on fa3024105736:40255 in memory (size: 29.2 KiB, free: 13.8 GiB)\n",
      "24/08/13 22:52:16 INFO BlockManagerInfo: Removed broadcast_30_piece0 on fa3024105736:40255 in memory (size: 7.9 KiB, free: 13.8 GiB)\n",
      "24/08/13 22:52:16 INFO BlockManagerInfo: Removed broadcast_9_piece0 on fa3024105736:40255 in memory (size: 7.9 KiB, free: 13.8 GiB)\n",
      "24/08/13 22:52:16 INFO SnapshotScan: Scanning table local.uk_gender_pay_data snapshot 3775120664731591 created at 2024-08-13T22:49:08.730+00:00 with filter ResponsiblePerson IS NULL\n",
      "24/08/13 22:52:16 INFO BlockManagerInfo: Removed broadcast_11_piece0 on fa3024105736:40255 in memory (size: 29.2 KiB, free: 13.8 GiB)\n",
      "24/08/13 22:52:16 INFO BlockManagerInfo: Removed broadcast_20_piece0 on fa3024105736:40255 in memory (size: 29.3 KiB, free: 13.8 GiB)\n",
      "24/08/13 22:52:16 INFO BlockManagerInfo: Removed broadcast_5_piece0 on fa3024105736:40255 in memory (size: 29.1 KiB, free: 13.8 GiB)\n",
      "24/08/13 22:52:16 INFO BlockManagerInfo: Removed broadcast_19_piece0 on fa3024105736:40255 in memory (size: 29.4 KiB, free: 13.8 GiB)\n",
      "24/08/13 22:52:16 INFO BlockManagerInfo: Removed broadcast_4_piece0 on fa3024105736:40255 in memory (size: 34.4 KiB, free: 13.8 GiB)\n",
      "24/08/13 22:52:16 INFO LoggingMetricsReporter: Received metrics report: ScanReport{tableName=local.uk_gender_pay_data, snapshotId=3775120664731591, filter=is_null(ref(name=\"ResponsiblePerson\")), schemaId=0, projectedFieldIds=[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27], projectedFieldNames=[EmployerName, EmployerId, Address, PostCode, CompanyNumber, SicCodes, DiffMeanHourlyPercent, DiffMedianHourlyPercent, DiffMeanBonusPercent, DiffMedianBonusPercent, MaleBonusPercent, FemaleBonusPercent, MaleLowerQuartile, FemaleLowerQuartile, MaleLowerMiddleQuartile, FemaleLowerMiddleQuartile, MaleUpperMiddleQuartile, FemaleUpperMiddleQuartile, MaleTopQuartile, FemaleTopQuartile, CompanyLinkToGPGInfo, ResponsiblePerson, EmployerSize, CurrentName, SubmittedAfterTheDeadline, DueDate, DateSubmitted], scanMetrics=ScanMetricsResult{totalPlanningDuration=TimerResult{timeUnit=NANOSECONDS, totalDuration=PT0.016276171S, count=1}, resultDataFiles=CounterResult{unit=COUNT, value=1}, resultDeleteFiles=CounterResult{unit=COUNT, value=0}, totalDataManifests=CounterResult{unit=COUNT, value=1}, totalDeleteManifests=CounterResult{unit=COUNT, value=0}, scannedDataManifests=CounterResult{unit=COUNT, value=1}, skippedDataManifests=CounterResult{unit=COUNT, value=0}, totalFileSizeInBytes=CounterResult{unit=BYTES, value=1294005}, totalDeleteFileSizeInBytes=CounterResult{unit=BYTES, value=0}, skippedDataFiles=CounterResult{unit=COUNT, value=0}, skippedDeleteFiles=CounterResult{unit=COUNT, value=0}, scannedDeleteManifests=CounterResult{unit=COUNT, value=0}, skippedDeleteManifests=CounterResult{unit=COUNT, value=0}, indexedDeleteFiles=CounterResult{unit=COUNT, value=0}, equalityDeleteFiles=CounterResult{unit=COUNT, value=0}, positionalDeleteFiles=CounterResult{unit=COUNT, value=0}}, metadata={engine-version=3.5.2, iceberg-version=Apache Iceberg 1.6.0 (commit 229d8f6fcd109e6c8943ea7cbb41dab746c6d0ed), app-id=local-1723589343398, engine-name=spark}}\n",
      "24/08/13 22:52:16 INFO GroupBasedRowLevelOperationScanPlanning: \n",
      "Pushing operators to local.uk_gender_pay_data\n",
      "Pushed filters: ResponsiblePerson IS NULL\n",
      "Filters evaluated on data source side: \n",
      "Filters evaluated on Spark side: isnull(ResponsiblePerson#1024)\n",
      "Output: EmployerName#1003, EmployerId#1004, Address#1005, PostCode#1006, CompanyNumber#1007, SicCodes#1008, DiffMeanHourlyPercent#1009, DiffMedianHourlyPercent#1010, DiffMeanBonusPercent#1011, DiffMedianBonusPercent#1012, MaleBonusPercent#1013, FemaleBonusPercent#1014, MaleLowerQuartile#1015, FemaleLowerQuartile#1016, MaleLowerMiddleQuartile#1017, FemaleLowerMiddleQuartile#1018, MaleUpperMiddleQuartile#1019, FemaleUpperMiddleQuartile#1020, MaleTopQuartile#1021, FemaleTopQuartile#1022, CompanyLinkToGPGInfo#1023, ResponsiblePerson#1024, EmployerSize#1025, CurrentName#1026, SubmittedAfterTheDeadline#1027, DueDate#1028, DateSubmitted#1029, _file#1032, _pos#1033L\n",
      "         \n",
      "24/08/13 22:52:16 INFO SnapshotScan: Scanning table local.uk_gender_pay_data snapshot 3775120664731591 created at 2024-08-13T22:49:08.730+00:00 with filter ResponsiblePerson IS NULL\n",
      "24/08/13 22:52:17 INFO SparkPartitioningAwareScan: Reporting UnknownPartitioning with 1 partition(s) for table local.uk_gender_pay_data\n",
      "24/08/13 22:52:17 INFO SparkWrite: Requesting 402653184 bytes advisory partition size for table local.uk_gender_pay_data\n",
      "24/08/13 22:52:17 INFO SparkWrite: Requesting ClusteredDistribution(_file) as write distribution for table local.uk_gender_pay_data\n",
      "24/08/13 22:52:17 INFO SparkWrite: Requesting [] as write ordering for table local.uk_gender_pay_data\n",
      "24/08/13 22:52:17 INFO V2ScanRelationPushDown: \n",
      "Pushing operators to local.uk_gender_pay_data\n",
      "Pushed Filters: ResponsiblePerson IS NULL\n",
      "Post-Scan Filters: isnull(ResponsiblePerson#1114)\n",
      "         \n",
      "24/08/13 22:52:17 INFO V2ScanRelationPushDown: \n",
      "Output: ResponsiblePerson#1114, _file#1120\n",
      "         \n",
      "24/08/13 22:52:17 INFO SnapshotScan: Scanning table local.uk_gender_pay_data snapshot 3775120664731591 created at 2024-08-13T22:49:08.730+00:00 with filter ResponsiblePerson IS NULL\n",
      "24/08/13 22:52:17 INFO BaseDistributedDataScan: Planning file tasks locally for table local.uk_gender_pay_data\n",
      "24/08/13 22:52:17 INFO SparkPartitioningAwareScan: Reporting UnknownPartitioning with 1 partition(s) for table local.uk_gender_pay_data\n",
      "24/08/13 22:52:17 INFO MemoryStore: Block broadcast_31 stored as values in memory (estimated size 32.0 KiB, free 13.8 GiB)\n",
      "24/08/13 22:52:17 INFO MemoryStore: Block broadcast_31_piece0 stored as bytes in memory (estimated size 29.4 KiB, free 13.8 GiB)\n",
      "24/08/13 22:52:17 INFO BlockManagerInfo: Added broadcast_31_piece0 in memory on fa3024105736:40255 (size: 29.4 KiB, free: 13.8 GiB)\n",
      "24/08/13 22:52:17 INFO SparkContext: Created broadcast 31 from broadcast at SparkBatch.java:79\n",
      "24/08/13 22:52:17 INFO MemoryStore: Block broadcast_32 stored as values in memory (estimated size 32.0 KiB, free 13.8 GiB)\n",
      "24/08/13 22:52:17 INFO MemoryStore: Block broadcast_32_piece0 stored as bytes in memory (estimated size 29.3 KiB, free 13.8 GiB)\n",
      "24/08/13 22:52:17 INFO BlockManagerInfo: Added broadcast_32_piece0 in memory on fa3024105736:40255 (size: 29.3 KiB, free: 13.8 GiB)\n",
      "24/08/13 22:52:17 INFO SparkContext: Created broadcast 32 from broadcast at SparkBatch.java:79\n",
      "24/08/13 22:52:17 INFO MemoryStore: Block broadcast_33 stored as values in memory (estimated size 32.0 KiB, free 13.8 GiB)\n",
      "24/08/13 22:52:17 INFO MemoryStore: Block broadcast_33_piece0 stored as bytes in memory (estimated size 29.4 KiB, free 13.8 GiB)\n",
      "24/08/13 22:52:17 INFO BlockManagerInfo: Added broadcast_33_piece0 in memory on fa3024105736:40255 (size: 29.4 KiB, free: 13.8 GiB)\n",
      "24/08/13 22:52:17 INFO SparkContext: Created broadcast 33 from broadcast at SparkBatch.java:79\n",
      "24/08/13 22:52:17 INFO MemoryStore: Block broadcast_34 stored as values in memory (estimated size 32.0 KiB, free 13.8 GiB)\n",
      "24/08/13 22:52:17 INFO MemoryStore: Block broadcast_34_piece0 stored as bytes in memory (estimated size 29.3 KiB, free 13.8 GiB)\n",
      "24/08/13 22:52:17 INFO BlockManagerInfo: Added broadcast_34_piece0 in memory on fa3024105736:40255 (size: 29.3 KiB, free: 13.8 GiB)\n",
      "24/08/13 22:52:17 INFO SparkContext: Created broadcast 34 from broadcast at SparkBatch.java:79\n",
      "24/08/13 22:52:17 INFO CodeGenerator: Code generated in 15.515653 ms\n",
      "24/08/13 22:52:17 INFO CodeGenerator: Code generated in 58.417965 ms\n",
      "24/08/13 22:52:17 INFO DAGScheduler: Registering RDD 47 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) as input to shuffle 0\n",
      "24/08/13 22:52:17 INFO DAGScheduler: Got map stage job 11 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) with 1 output partitions\n",
      "24/08/13 22:52:17 INFO DAGScheduler: Final stage: ShuffleMapStage 11 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264)\n",
      "24/08/13 22:52:17 INFO DAGScheduler: Parents of final stage: List()\n",
      "24/08/13 22:52:17 INFO DAGScheduler: Missing parents: List()\n",
      "24/08/13 22:52:17 INFO DAGScheduler: Submitting ShuffleMapStage 11 (MapPartitionsRDD[47] at $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264), which has no missing parents\n",
      "24/08/13 22:52:17 INFO MemoryStore: Block broadcast_35 stored as values in memory (estimated size 30.8 KiB, free 13.8 GiB)\n",
      "24/08/13 22:52:17 INFO MemoryStore: Block broadcast_35_piece0 stored as bytes in memory (estimated size 12.8 KiB, free 13.8 GiB)\n",
      "24/08/13 22:52:17 INFO BlockManagerInfo: Added broadcast_35_piece0 in memory on fa3024105736:40255 (size: 12.8 KiB, free: 13.8 GiB)\n",
      "24/08/13 22:52:17 INFO SparkContext: Created broadcast 35 from broadcast at DAGScheduler.scala:1585\n",
      "24/08/13 22:52:17 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 11 (MapPartitionsRDD[47] at $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) (first 15 tasks are for partitions Vector(0))\n",
      "24/08/13 22:52:17 INFO TaskSchedulerImpl: Adding task set 11.0 with 1 tasks resource profile 0\n",
      "24/08/13 22:52:17 INFO TaskSetManager: Starting task 0.0 in stage 11.0 (TID 11) (fa3024105736, executor driver, partition 0, PROCESS_LOCAL, 15576 bytes) \n",
      "24/08/13 22:52:17 INFO Executor: Running task 0.0 in stage 11.0 (TID 11)\n",
      "24/08/13 22:52:17 INFO CodeGenerator: Code generated in 29.071454 ms\n",
      "24/08/13 22:52:17 INFO CodeGenerator: Code generated in 10.372302 ms\n",
      "24/08/13 22:52:17 INFO CodeGenerator: Code generated in 4.646838 ms\n",
      "24/08/13 22:52:17 INFO CodecPool: Got brand-new decompressor [.zstd]\n",
      "24/08/13 22:52:17 INFO Executor: Finished task 0.0 in stage 11.0 (TID 11). 5588 bytes result sent to driver\n",
      "24/08/13 22:52:17 INFO TaskSetManager: Finished task 0.0 in stage 11.0 (TID 11) in 156 ms on fa3024105736 (executor driver) (1/1)\n",
      "24/08/13 22:52:17 INFO TaskSchedulerImpl: Removed TaskSet 11.0, whose tasks have all completed, from pool \n",
      "24/08/13 22:52:17 INFO DAGScheduler: ShuffleMapStage 11 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) finished in 0.171 s\n",
      "24/08/13 22:52:17 INFO DAGScheduler: looking for newly runnable stages\n",
      "24/08/13 22:52:17 INFO DAGScheduler: running: HashSet()\n",
      "24/08/13 22:52:17 INFO DAGScheduler: waiting: HashSet()\n",
      "24/08/13 22:52:17 INFO DAGScheduler: failed: HashSet()\n",
      "24/08/13 22:52:17 INFO ShufflePartitionsUtil: For shuffle(0), advisory target size: 67108864, actual target size 1048576, minimum partition size: 1048576\n",
      "24/08/13 22:52:17 INFO CodeGenerator: Code generated in 17.976194 ms\n",
      "24/08/13 22:52:17 INFO SparkContext: Starting job: $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264\n",
      "24/08/13 22:52:17 INFO DAGScheduler: Got job 12 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) with 1 output partitions\n",
      "24/08/13 22:52:17 INFO DAGScheduler: Final stage: ResultStage 13 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264)\n",
      "24/08/13 22:52:17 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 12)\n",
      "24/08/13 22:52:17 INFO DAGScheduler: Missing parents: List()\n",
      "24/08/13 22:52:17 INFO DAGScheduler: Submitting ResultStage 13 (MapPartitionsRDD[50] at $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264), which has no missing parents\n",
      "24/08/13 22:52:17 INFO MemoryStore: Block broadcast_36 stored as values in memory (estimated size 33.8 KiB, free 13.8 GiB)\n",
      "24/08/13 22:52:17 INFO MemoryStore: Block broadcast_36_piece0 stored as bytes in memory (estimated size 14.7 KiB, free 13.8 GiB)\n",
      "24/08/13 22:52:17 INFO BlockManagerInfo: Added broadcast_36_piece0 in memory on fa3024105736:40255 (size: 14.7 KiB, free: 13.8 GiB)\n",
      "24/08/13 22:52:17 INFO SparkContext: Created broadcast 36 from broadcast at DAGScheduler.scala:1585\n",
      "24/08/13 22:52:17 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 13 (MapPartitionsRDD[50] at $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) (first 15 tasks are for partitions Vector(0))\n",
      "24/08/13 22:52:17 INFO TaskSchedulerImpl: Adding task set 13.0 with 1 tasks resource profile 0\n",
      "24/08/13 22:52:17 INFO TaskSetManager: Starting task 0.0 in stage 13.0 (TID 12) (fa3024105736, executor driver, partition 0, NODE_LOCAL, 9079 bytes) \n",
      "24/08/13 22:52:17 INFO Executor: Running task 0.0 in stage 13.0 (TID 12)\n",
      "24/08/13 22:52:17 INFO ShuffleBlockFetcherIterator: Getting 1 (189.0 B) non-empty blocks including 1 (189.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
      "24/08/13 22:52:17 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 12 ms\n",
      "24/08/13 22:52:17 INFO CodeGenerator: Code generated in 13.459901 ms\n",
      "24/08/13 22:52:17 INFO Executor: Finished task 0.0 in stage 13.0 (TID 12). 8064 bytes result sent to driver\n",
      "24/08/13 22:52:17 INFO TaskSetManager: Finished task 0.0 in stage 13.0 (TID 12) in 71 ms on fa3024105736 (executor driver) (1/1)\n",
      "24/08/13 22:52:17 INFO TaskSchedulerImpl: Removed TaskSet 13.0, whose tasks have all completed, from pool \n",
      "24/08/13 22:52:17 INFO DAGScheduler: ResultStage 13 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) finished in 0.080 s\n",
      "24/08/13 22:52:17 INFO DAGScheduler: Job 12 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "24/08/13 22:52:17 INFO TaskSchedulerImpl: Killing all running tasks in stage 13: Stage finished\n",
      "24/08/13 22:52:17 INFO DAGScheduler: Job 12 finished: $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264, took 0.089954 s\n",
      "24/08/13 22:52:17 INFO MemoryStore: Block broadcast_37 stored as values in memory (estimated size 240.0 B, free 13.8 GiB)\n",
      "24/08/13 22:52:17 INFO MemoryStore: Block broadcast_37_piece0 stored as bytes in memory (estimated size 275.0 B, free 13.8 GiB)\n",
      "24/08/13 22:52:17 INFO BlockManagerInfo: Added broadcast_37_piece0 in memory on fa3024105736:40255 (size: 275.0 B, free: 13.8 GiB)\n",
      "24/08/13 22:52:17 INFO SparkContext: Created broadcast 37 from sql at cmd21.sc:1\n",
      "24/08/13 22:52:17 INFO SparkCopyOnWriteScan: 1 of 1 task(s) for table local.uk_gender_pay_data matched runtime file filter with 1 location(s)\n",
      "24/08/13 22:52:17 INFO MemoryStore: Block broadcast_38 stored as values in memory (estimated size 32.0 KiB, free 13.8 GiB)\n",
      "24/08/13 22:52:17 INFO MemoryStore: Block broadcast_38_piece0 stored as bytes in memory (estimated size 29.4 KiB, free 13.8 GiB)\n",
      "24/08/13 22:52:17 INFO BlockManagerInfo: Added broadcast_38_piece0 in memory on fa3024105736:40255 (size: 29.4 KiB, free: 13.8 GiB)\n",
      "24/08/13 22:52:17 INFO SparkContext: Created broadcast 38 from broadcast at SparkBatch.java:79\n",
      "24/08/13 22:52:17 INFO DAGScheduler: Registering RDD 54 (sql at cmd21.sc:1) as input to shuffle 1\n",
      "24/08/13 22:52:17 INFO DAGScheduler: Got map stage job 13 (sql at cmd21.sc:1) with 1 output partitions\n",
      "24/08/13 22:52:17 INFO DAGScheduler: Final stage: ShuffleMapStage 14 (sql at cmd21.sc:1)\n",
      "24/08/13 22:52:17 INFO DAGScheduler: Parents of final stage: List()\n",
      "24/08/13 22:52:17 INFO DAGScheduler: Missing parents: List()\n",
      "24/08/13 22:52:17 INFO DAGScheduler: Submitting ShuffleMapStage 14 (MapPartitionsRDD[54] at sql at cmd21.sc:1), which has no missing parents\n",
      "24/08/13 22:52:17 INFO MemoryStore: Block broadcast_39 stored as values in memory (estimated size 35.0 KiB, free 13.8 GiB)\n",
      "24/08/13 22:52:17 INFO MemoryStore: Block broadcast_39_piece0 stored as bytes in memory (estimated size 10.4 KiB, free 13.8 GiB)\n",
      "24/08/13 22:52:17 INFO BlockManagerInfo: Added broadcast_39_piece0 in memory on fa3024105736:40255 (size: 10.4 KiB, free: 13.8 GiB)\n",
      "24/08/13 22:52:17 INFO SparkContext: Created broadcast 39 from broadcast at DAGScheduler.scala:1585\n",
      "24/08/13 22:52:17 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 14 (MapPartitionsRDD[54] at sql at cmd21.sc:1) (first 15 tasks are for partitions Vector(0))\n",
      "24/08/13 22:52:17 INFO TaskSchedulerImpl: Adding task set 14.0 with 1 tasks resource profile 0\n",
      "24/08/13 22:52:17 INFO TaskSetManager: Starting task 0.0 in stage 14.0 (TID 13) (fa3024105736, executor driver, partition 0, PROCESS_LOCAL, 17232 bytes) \n",
      "24/08/13 22:52:17 INFO Executor: Running task 0.0 in stage 14.0 (TID 13)\n",
      "24/08/13 22:52:17 INFO CodeGenerator: Code generated in 9.713445 ms\n",
      "24/08/13 22:52:17 INFO CodeGenerator: Code generated in 6.211016 ms\n",
      "24/08/13 22:52:17 INFO CodecPool: Got brand-new decompressor [.zstd]\n",
      "24/08/13 22:52:17 INFO Executor: Finished task 0.0 in stage 14.0 (TID 13). 5043 bytes result sent to driver\n",
      "24/08/13 22:52:17 INFO TaskSetManager: Finished task 0.0 in stage 14.0 (TID 13) in 109 ms on fa3024105736 (executor driver) (1/1)\n",
      "24/08/13 22:52:17 INFO TaskSchedulerImpl: Removed TaskSet 14.0, whose tasks have all completed, from pool \n",
      "24/08/13 22:52:17 INFO DAGScheduler: ShuffleMapStage 14 (sql at cmd21.sc:1) finished in 0.118 s\n",
      "24/08/13 22:52:17 INFO DAGScheduler: looking for newly runnable stages\n",
      "24/08/13 22:52:17 INFO DAGScheduler: running: HashSet()\n",
      "24/08/13 22:52:17 INFO DAGScheduler: waiting: HashSet()\n",
      "24/08/13 22:52:17 INFO DAGScheduler: failed: HashSet()\n",
      "24/08/13 22:52:17 INFO ShufflePartitionsUtil: For shuffle(1), advisory target size: 402653184, actual target size 3152474, minimum partition size: 1048576\n",
      "24/08/13 22:52:17 INFO CodeGenerator: Code generated in 6.779694 ms\n",
      "24/08/13 22:52:17 INFO MemoryStore: Block broadcast_40 stored as values in memory (estimated size 32.0 KiB, free 13.8 GiB)\n",
      "24/08/13 22:52:17 INFO MemoryStore: Block broadcast_40_piece0 stored as bytes in memory (estimated size 29.3 KiB, free 13.8 GiB)\n",
      "24/08/13 22:52:17 INFO BlockManagerInfo: Added broadcast_40_piece0 in memory on fa3024105736:40255 (size: 29.3 KiB, free: 13.8 GiB)\n",
      "24/08/13 22:52:17 INFO SparkContext: Created broadcast 40 from broadcast at SparkWrite.java:193\n",
      "24/08/13 22:52:17 INFO ReplaceDataExec: Start processing data source write support: IcebergBatchWrite(table=local.uk_gender_pay_data, format=PARQUET). The input RDD has 1 partitions.\n",
      "24/08/13 22:52:17 INFO SparkContext: Starting job: sql at cmd21.sc:1\n",
      "24/08/13 22:52:17 INFO DAGScheduler: Got job 14 (sql at cmd21.sc:1) with 1 output partitions\n",
      "24/08/13 22:52:17 INFO DAGScheduler: Final stage: ResultStage 16 (sql at cmd21.sc:1)\n",
      "24/08/13 22:52:17 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 15)\n",
      "24/08/13 22:52:17 INFO DAGScheduler: Missing parents: List()\n",
      "24/08/13 22:52:17 INFO DAGScheduler: Submitting ResultStage 16 (MapPartitionsRDD[56] at sql at cmd21.sc:1), which has no missing parents\n",
      "24/08/13 22:52:17 INFO MemoryStore: Block broadcast_41 stored as values in memory (estimated size 24.0 KiB, free 13.8 GiB)\n",
      "24/08/13 22:52:17 INFO MemoryStore: Block broadcast_41_piece0 stored as bytes in memory (estimated size 9.0 KiB, free 13.8 GiB)\n",
      "24/08/13 22:52:17 INFO BlockManagerInfo: Added broadcast_41_piece0 in memory on fa3024105736:40255 (size: 9.0 KiB, free: 13.8 GiB)\n",
      "24/08/13 22:52:17 INFO SparkContext: Created broadcast 41 from broadcast at DAGScheduler.scala:1585\n",
      "24/08/13 22:52:17 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 16 (MapPartitionsRDD[56] at sql at cmd21.sc:1) (first 15 tasks are for partitions Vector(0))\n",
      "24/08/13 22:52:17 INFO TaskSchedulerImpl: Adding task set 16.0 with 1 tasks resource profile 0\n",
      "24/08/13 22:52:17 INFO TaskSetManager: Starting task 0.0 in stage 16.0 (TID 14) (fa3024105736, executor driver, partition 0, NODE_LOCAL, 9079 bytes) \n",
      "24/08/13 22:52:17 INFO Executor: Running task 0.0 in stage 16.0 (TID 14)\n",
      "24/08/13 22:52:17 INFO ShuffleBlockFetcherIterator: Getting 1 (3.0 MiB) non-empty blocks including 1 (3.0 MiB) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
      "24/08/13 22:52:17 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms\n",
      "24/08/13 22:52:17 INFO CodeGenerator: Code generated in 7.058678 ms\n",
      "24/08/13 22:52:17 INFO CodecPool: Got brand-new compressor [.zstd]\n",
      "24/08/13 22:52:18 INFO DataWritingSparkTask: Writer for partition 0 is committing.\n",
      "24/08/13 22:52:18 INFO DataWritingSparkTask: Committed partition 0 (task 14, attempt 0, stage 16.0)\n",
      "24/08/13 22:52:18 INFO Executor: Finished task 0.0 in stage 16.0 (TID 14). 9115 bytes result sent to driver\n",
      "24/08/13 22:52:18 INFO TaskSetManager: Finished task 0.0 in stage 16.0 (TID 14) in 208 ms on fa3024105736 (executor driver) (1/1)\n",
      "24/08/13 22:52:18 INFO TaskSchedulerImpl: Removed TaskSet 16.0, whose tasks have all completed, from pool \n",
      "24/08/13 22:52:18 INFO DAGScheduler: ResultStage 16 (sql at cmd21.sc:1) finished in 0.217 s\n",
      "24/08/13 22:52:18 INFO DAGScheduler: Job 14 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "24/08/13 22:52:18 INFO TaskSchedulerImpl: Killing all running tasks in stage 16: Stage finished\n",
      "24/08/13 22:52:18 INFO DAGScheduler: Job 14 finished: sql at cmd21.sc:1, took 0.219578 s\n",
      "24/08/13 22:52:18 INFO ReplaceDataExec: Data source write support IcebergBatchWrite(table=local.uk_gender_pay_data, format=PARQUET) is committing.\n",
      "24/08/13 22:52:18 INFO SparkWrite: Committing overwrite of 1 data files with 1 new data files, scanSnapshotId: 3775120664731591, conflictDetectionFilter: is_null(ref(name=\"ResponsiblePerson\")) to table local.uk_gender_pay_data\n",
      "24/08/13 22:52:18 INFO HadoopTableOperations: Committed a new metadata file /high-performance-spark-examples/warehouse/uk_gender_pay_data/metadata/v2.metadata.json\n",
      "24/08/13 22:52:18 INFO SnapshotProducer: Committed snapshot 4332408363965382155 (BaseOverwriteFiles)\n",
      "24/08/13 22:52:18 INFO LoggingMetricsReporter: Received metrics report: CommitReport{tableName=local.uk_gender_pay_data, snapshotId=4332408363965382155, sequenceNumber=2, operation=overwrite, commitMetrics=CommitMetricsResult{totalDuration=TimerResult{timeUnit=NANOSECONDS, totalDuration=PT0.112533239S, count=1}, attempts=CounterResult{unit=COUNT, value=1}, addedDataFiles=CounterResult{unit=COUNT, value=1}, removedDataFiles=CounterResult{unit=COUNT, value=1}, totalDataFiles=CounterResult{unit=COUNT, value=1}, addedDeleteFiles=null, addedEqualityDeleteFiles=null, addedPositionalDeleteFiles=null, removedDeleteFiles=null, removedEqualityDeleteFiles=null, removedPositionalDeleteFiles=null, totalDeleteFiles=CounterResult{unit=COUNT, value=0}, addedRecords=CounterResult{unit=COUNT, value=9984}, removedRecords=CounterResult{unit=COUNT, value=15395}, totalRecords=CounterResult{unit=COUNT, value=9984}, addedFilesSizeInBytes=CounterResult{unit=BYTES, value=1049668}, removedFilesSizeInBytes=CounterResult{unit=BYTES, value=1294005}, totalFilesSizeInBytes=CounterResult{unit=BYTES, value=1049668}, addedPositionalDeletes=null, removedPositionalDeletes=null, totalPositionalDeletes=CounterResult{unit=COUNT, value=0}, addedEqualityDeletes=null, removedEqualityDeletes=null, totalEqualityDeletes=CounterResult{unit=COUNT, value=0}}, metadata={engine-version=3.5.2, app-id=local-1723589343398, engine-name=spark, iceberg-version=Apache Iceberg 1.6.0 (commit 229d8f6fcd109e6c8943ea7cbb41dab746c6d0ed)}}\n",
      "24/08/13 22:52:18 INFO SparkWrite: Committed in 113 ms\n",
      "24/08/13 22:52:18 INFO ReplaceDataExec: Data source write support IcebergBatchWrite(table=local.uk_gender_pay_data, format=PARQUET) committed.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[36mres21\u001b[39m: \u001b[32mDataFrame\u001b[39m = []"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"DELETE FROM local.uk_gender_pay_data WHERE isnull(responsibleperson)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "73d279f3-f2a5-4ddf-a56f-d473b0c28b97",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/08/13 22:52:18 INFO V2ScanRelationPushDown: \n",
      "Pushing operators to local.uk_gender_pay_data\n",
      "Pushed Filters: ResponsiblePerson IS NULL\n",
      "Post-Scan Filters: isnull(ResponsiblePerson#1146)\n",
      "         \n",
      "24/08/13 22:52:18 INFO V2ScanRelationPushDown: \n",
      "Output: EmployerName#1125, EmployerId#1126, Address#1127, PostCode#1128, CompanyNumber#1129, SicCodes#1130, DiffMeanHourlyPercent#1131, DiffMedianHourlyPercent#1132, DiffMeanBonusPercent#1133, DiffMedianBonusPercent#1134, MaleBonusPercent#1135, FemaleBonusPercent#1136, MaleLowerQuartile#1137, FemaleLowerQuartile#1138, MaleLowerMiddleQuartile#1139, FemaleLowerMiddleQuartile#1140, MaleUpperMiddleQuartile#1141, FemaleUpperMiddleQuartile#1142, MaleTopQuartile#1143, FemaleTopQuartile#1144, CompanyLinkToGPGInfo#1145, ResponsiblePerson#1146, EmployerSize#1147, CurrentName#1148, SubmittedAfterTheDeadline#1149, DueDate#1150, DateSubmitted#1151\n",
      "         \n",
      "24/08/13 22:52:18 INFO SnapshotScan: Scanning table local.uk_gender_pay_data snapshot 4332408363965382155 created at 2024-08-13T22:52:18.219+00:00 with filter ResponsiblePerson IS NULL\n",
      "24/08/13 22:52:18 INFO BaseDistributedDataScan: Planning file tasks locally for table local.uk_gender_pay_data\n",
      "24/08/13 22:52:18 INFO SparkPartitioningAwareScan: Reporting UnknownPartitioning with 0 partition(s) for table local.uk_gender_pay_data\n",
      "24/08/13 22:52:18 INFO MemoryStore: Block broadcast_42 stored as values in memory (estimated size 32.0 KiB, free 13.8 GiB)\n",
      "24/08/13 22:52:18 INFO MemoryStore: Block broadcast_42_piece0 stored as bytes in memory (estimated size 29.4 KiB, free 13.8 GiB)\n",
      "24/08/13 22:52:18 INFO BlockManagerInfo: Added broadcast_42_piece0 in memory on fa3024105736:40255 (size: 29.4 KiB, free: 13.8 GiB)\n",
      "24/08/13 22:52:18 INFO SparkContext: Created broadcast 42 from broadcast at SparkBatch.java:79\n",
      "24/08/13 22:52:18 INFO MemoryStore: Block broadcast_43 stored as values in memory (estimated size 32.0 KiB, free 13.8 GiB)\n",
      "24/08/13 22:52:18 INFO MemoryStore: Block broadcast_43_piece0 stored as bytes in memory (estimated size 29.3 KiB, free 13.8 GiB)\n",
      "24/08/13 22:52:18 INFO BlockManagerInfo: Added broadcast_43_piece0 in memory on fa3024105736:40255 (size: 29.3 KiB, free: 13.8 GiB)\n",
      "24/08/13 22:52:18 INFO SparkContext: Created broadcast 43 from broadcast at SparkBatch.java:79\n",
      "24/08/13 22:52:18 INFO CodeGenerator: Code generated in 11.59045 ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+----------+-------+--------+-------------+--------+---------------------+-----------------------+--------------------+----------------------+----------------+------------------+-----------------+-------------------+-----------------------+-------------------------+-----------------------+-------------------------+---------------+-----------------+--------------------+-----------------+------------+-----------+-------------------------+-------+-------------+\n",
      "|EmployerName|EmployerId|Address|PostCode|CompanyNumber|SicCodes|DiffMeanHourlyPercent|DiffMedianHourlyPercent|DiffMeanBonusPercent|DiffMedianBonusPercent|MaleBonusPercent|FemaleBonusPercent|MaleLowerQuartile|FemaleLowerQuartile|MaleLowerMiddleQuartile|FemaleLowerMiddleQuartile|MaleUpperMiddleQuartile|FemaleUpperMiddleQuartile|MaleTopQuartile|FemaleTopQuartile|CompanyLinkToGPGInfo|ResponsiblePerson|EmployerSize|CurrentName|SubmittedAfterTheDeadline|DueDate|DateSubmitted|\n",
      "+------------+----------+-------+--------+-------------+--------+---------------------+-----------------------+--------------------+----------------------+----------------+------------------+-----------------+-------------------+-----------------------+-------------------------+-----------------------+-------------------------+---------------+-----------------+--------------------+-----------------+------------+-----------+-------------------------+-------+-------------+\n",
      "+------------+----------+-------+--------+-------------+--------+---------------------+-----------------------+--------------------+----------------------+----------------+------------------+-----------------+-------------------+-----------------------+-------------------------+-----------------------+-------------------------+---------------+-----------------+--------------------+-----------------+------------+-----------+-------------------------+-------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "// Make sure the data is gone\n",
    "spark.sql(\"SELECT * FROM local.uk_gender_pay_data WHERE isnull(responsibleperson) LIMIT 5\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9b6902ef-b742-466d-b4c8-d6830ff67cf4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/08/13 22:52:18 INFO V2ScanRelationPushDown: \n",
      "Pushing operators to local.uk_gender_pay_data\n",
      "Pushed Filters: ResponsiblePerson IS NULL\n",
      "Post-Scan Filters: isnull(ResponsiblePerson#1341)\n",
      "         \n",
      "24/08/13 22:52:18 INFO V2ScanRelationPushDown: \n",
      "Output: EmployerName#1320, EmployerId#1321, Address#1322, PostCode#1323, CompanyNumber#1324, SicCodes#1325, DiffMeanHourlyPercent#1326, DiffMedianHourlyPercent#1327, DiffMeanBonusPercent#1328, DiffMedianBonusPercent#1329, MaleBonusPercent#1330, FemaleBonusPercent#1331, MaleLowerQuartile#1332, FemaleLowerQuartile#1333, MaleLowerMiddleQuartile#1334, FemaleLowerMiddleQuartile#1335, MaleUpperMiddleQuartile#1336, FemaleUpperMiddleQuartile#1337, MaleTopQuartile#1338, FemaleTopQuartile#1339, CompanyLinkToGPGInfo#1340, ResponsiblePerson#1341, EmployerSize#1342, CurrentName#1343, SubmittedAfterTheDeadline#1344, DueDate#1345, DateSubmitted#1346\n",
      "         \n",
      "24/08/13 22:52:18 INFO SnapshotScan: Scanning table local.uk_gender_pay_data snapshot 3775120664731591 created at 2024-08-13T22:49:08.730+00:00 with filter ResponsiblePerson IS NULL\n",
      "24/08/13 22:52:18 INFO BaseDistributedDataScan: Planning file tasks locally for table local.uk_gender_pay_data\n",
      "24/08/13 22:52:18 INFO SparkPartitioningAwareScan: Reporting UnknownPartitioning with 1 partition(s) for table local.uk_gender_pay_data\n",
      "24/08/13 22:52:18 INFO MemoryStore: Block broadcast_44 stored as values in memory (estimated size 32.0 KiB, free 13.8 GiB)\n",
      "24/08/13 22:52:18 INFO MemoryStore: Block broadcast_44_piece0 stored as bytes in memory (estimated size 29.4 KiB, free 13.8 GiB)\n",
      "24/08/13 22:52:18 INFO BlockManagerInfo: Added broadcast_44_piece0 in memory on fa3024105736:40255 (size: 29.4 KiB, free: 13.8 GiB)\n",
      "24/08/13 22:52:18 INFO SparkContext: Created broadcast 44 from broadcast at SparkBatch.java:79\n",
      "24/08/13 22:52:18 INFO MemoryStore: Block broadcast_45 stored as values in memory (estimated size 32.0 KiB, free 13.8 GiB)\n",
      "24/08/13 22:52:18 INFO MemoryStore: Block broadcast_45_piece0 stored as bytes in memory (estimated size 29.3 KiB, free 13.8 GiB)\n",
      "24/08/13 22:52:18 INFO BlockManagerInfo: Added broadcast_45_piece0 in memory on fa3024105736:40255 (size: 29.3 KiB, free: 13.8 GiB)\n",
      "24/08/13 22:52:18 INFO SparkContext: Created broadcast 45 from broadcast at SparkBatch.java:79\n",
      "24/08/13 22:52:18 INFO SparkContext: Starting job: show at cmd23.sc:1\n",
      "24/08/13 22:52:18 INFO DAGScheduler: Got job 15 (show at cmd23.sc:1) with 1 output partitions\n",
      "24/08/13 22:52:18 INFO DAGScheduler: Final stage: ResultStage 17 (show at cmd23.sc:1)\n",
      "24/08/13 22:52:18 INFO DAGScheduler: Parents of final stage: List()\n",
      "24/08/13 22:52:18 INFO DAGScheduler: Missing parents: List()\n",
      "24/08/13 22:52:18 INFO DAGScheduler: Submitting ResultStage 17 (MapPartitionsRDD[64] at show at cmd23.sc:1), which has no missing parents\n",
      "24/08/13 22:52:18 INFO MemoryStore: Block broadcast_46 stored as values in memory (estimated size 30.2 KiB, free 13.8 GiB)\n",
      "24/08/13 22:52:18 INFO MemoryStore: Block broadcast_46_piece0 stored as bytes in memory (estimated size 7.9 KiB, free 13.8 GiB)\n",
      "24/08/13 22:52:18 INFO BlockManagerInfo: Added broadcast_46_piece0 in memory on fa3024105736:40255 (size: 7.9 KiB, free: 13.8 GiB)\n",
      "24/08/13 22:52:18 INFO SparkContext: Created broadcast 46 from broadcast at DAGScheduler.scala:1585\n",
      "24/08/13 22:52:18 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 17 (MapPartitionsRDD[64] at show at cmd23.sc:1) (first 15 tasks are for partitions Vector(0))\n",
      "24/08/13 22:52:18 INFO TaskSchedulerImpl: Adding task set 17.0 with 1 tasks resource profile 0\n",
      "24/08/13 22:52:18 INFO TaskSetManager: Starting task 0.0 in stage 17.0 (TID 15) (fa3024105736, executor driver, partition 0, PROCESS_LOCAL, 17271 bytes) \n",
      "24/08/13 22:52:18 INFO Executor: Running task 0.0 in stage 17.0 (TID 15)\n",
      "24/08/13 22:52:18 INFO CodecPool: Got brand-new decompressor [.zstd]\n",
      "24/08/13 22:52:18 INFO Executor: Finished task 0.0 in stage 17.0 (TID 15). 5709 bytes result sent to driver\n",
      "24/08/13 22:52:18 INFO TaskSetManager: Finished task 0.0 in stage 17.0 (TID 15) in 23 ms on fa3024105736 (executor driver) (1/1)\n",
      "24/08/13 22:52:18 INFO TaskSchedulerImpl: Removed TaskSet 17.0, whose tasks have all completed, from pool \n",
      "24/08/13 22:52:18 INFO DAGScheduler: ResultStage 17 (show at cmd23.sc:1) finished in 0.028 s\n",
      "24/08/13 22:52:18 INFO DAGScheduler: Job 15 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "24/08/13 22:52:18 INFO TaskSchedulerImpl: Killing all running tasks in stage 17: Stage finished\n",
      "24/08/13 22:52:18 INFO DAGScheduler: Job 15 finished: show at cmd23.sc:1, took 0.032009 s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----------+--------------------+--------------------+-------------+--------+---------------------+-----------------------+--------------------+----------------------+----------------+------------------+-----------------+-------------------+-----------------------+-------------------------+-----------------------+-------------------------+---------------+-----------------+--------------------+-----------------+--------------------+-----------+-------------------------+---------+-------------+\n",
      "|        EmployerName|EmployerId|             Address|            PostCode|CompanyNumber|SicCodes|DiffMeanHourlyPercent|DiffMedianHourlyPercent|DiffMeanBonusPercent|DiffMedianBonusPercent|MaleBonusPercent|FemaleBonusPercent|MaleLowerQuartile|FemaleLowerQuartile|MaleLowerMiddleQuartile|FemaleLowerMiddleQuartile|MaleUpperMiddleQuartile|FemaleUpperMiddleQuartile|MaleTopQuartile|FemaleTopQuartile|CompanyLinkToGPGInfo|ResponsiblePerson|        EmployerSize|CurrentName|SubmittedAfterTheDeadline|  DueDate|DateSubmitted|\n",
      "+--------------------+----------+--------------------+--------------------+-------------+--------+---------------------+-----------------------+--------------------+----------------------+----------------+------------------+-----------------+-------------------+-----------------------+-------------------------+-----------------------+-------------------------+---------------+-----------------+--------------------+-----------------+--------------------+-----------+-------------------------+---------+-------------+\n",
      "|\"\"\"RED BAND\"\" CHE...|  LIMITED\"|               16879|19 Smith's Place,...|      EH6 8NU|SC016876|                47730|                    0.6|                -4.4|                  14.1|            -2.0|              21.8|             78.2|               41.0|                   59.0|                     23.2|                   76.8|                      4.9|           95.1|             20.5|                79.5|             NULL|Philip Galt (Mana...| 250 to 499|     \"\"\"RED BAND\"\" CHE...| LIMITED\"|        False|\n",
      "|          1509 GROUP|     15320|Royal Grammar Sch...|             GU1 3BB|     04104101|  85200,|                 NULL|                   NULL|                NULL|                  NULL|            NULL|              NULL|             NULL|               NULL|                   NULL|                     NULL|                   NULL|                     NULL|           NULL|             NULL|                NULL|             NULL|                NULL|       NULL|                     NULL|     NULL|         NULL|\n",
      "|1LIFE MANAGEMENT ...|       687|The Stables, Duxb...|             PR7 4AT|     02566586|  93110,|                 NULL|                   NULL|                NULL|                  NULL|            NULL|              NULL|             NULL|               NULL|                   NULL|                     NULL|                   NULL|                     NULL|           NULL|             NULL|                NULL|             NULL|                NULL|       NULL|                     NULL|     NULL|         NULL|\n",
      "|               93130|      NULL|                NULL|                NULL|         NULL|    NULL|                 NULL|                   NULL|                NULL|                  NULL|            NULL|              NULL|             NULL|               NULL|                   NULL|                     NULL|                   NULL|                     NULL|           NULL|             NULL|                NULL|             NULL|                NULL|       NULL|                     NULL|     NULL|         NULL|\n",
      "|  1ST HOME CARE LTD.|     17484|Suite 1, Ground F...|             G32 9AT|     SC272838|  86900,|                 NULL|                   NULL|                NULL|                  NULL|            NULL|              NULL|             NULL|               NULL|                   NULL|                     NULL|                   NULL|                     NULL|           NULL|             NULL|                NULL|             NULL|                NULL|       NULL|                     NULL|     NULL|         NULL|\n",
      "+--------------------+----------+--------------------+--------------------+-------------+--------+---------------------+-----------------------+--------------------+----------------------+----------------+------------------+-----------------+-------------------+-----------------------+-------------------------+-----------------------+-------------------------+---------------+-----------------+--------------------+-----------------+--------------------+-----------+-------------------------+---------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "// Yay! ok now lets travel back in time\n",
    "spark.sql(f\"SELECT * FROM local.uk_gender_pay_data VERSION AS OF ${snapshot} WHERE isnull(responsibleperson) LIMIT 5\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e7e899a1-d2cd-4e25-b142-e69fb9ca6774",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/08/13 22:52:18 INFO V2ScanRelationPushDown: \n",
      "Output: EmployerName#1515, EmployerId#1516, Address#1517, PostCode#1518, CompanyNumber#1519, SicCodes#1520, DiffMeanHourlyPercent#1521, DiffMedianHourlyPercent#1522, DiffMeanBonusPercent#1523, DiffMedianBonusPercent#1524, MaleBonusPercent#1525, FemaleBonusPercent#1526, MaleLowerQuartile#1527, FemaleLowerQuartile#1528, MaleLowerMiddleQuartile#1529, FemaleLowerMiddleQuartile#1530, MaleUpperMiddleQuartile#1531, FemaleUpperMiddleQuartile#1532, MaleTopQuartile#1533, FemaleTopQuartile#1534, CompanyLinkToGPGInfo#1535, ResponsiblePerson#1536, EmployerSize#1537, CurrentName#1538, SubmittedAfterTheDeadline#1539, DueDate#1540, DateSubmitted#1541\n",
      "         \n",
      "24/08/13 22:52:18 INFO SnapshotScan: Scanning table local.uk_gender_pay_data snapshot 3775120664731591 created at 2024-08-13T22:49:08.730+00:00 with filter true\n",
      "24/08/13 22:52:18 INFO BaseDistributedDataScan: Planning file tasks locally for table local.uk_gender_pay_data\n",
      "24/08/13 22:52:18 INFO SparkPartitioningAwareScan: Reporting UnknownPartitioning with 1 partition(s) for table local.uk_gender_pay_data\n",
      "24/08/13 22:52:18 INFO MemoryStore: Block broadcast_47 stored as values in memory (estimated size 32.0 KiB, free 13.8 GiB)\n",
      "24/08/13 22:52:18 INFO MemoryStore: Block broadcast_47_piece0 stored as bytes in memory (estimated size 29.4 KiB, free 13.8 GiB)\n",
      "24/08/13 22:52:18 INFO BlockManagerInfo: Added broadcast_47_piece0 in memory on fa3024105736:40255 (size: 29.4 KiB, free: 13.8 GiB)\n",
      "24/08/13 22:52:18 INFO SparkContext: Created broadcast 47 from broadcast at SparkBatch.java:79\n",
      "24/08/13 22:52:18 INFO MemoryStore: Block broadcast_48 stored as values in memory (estimated size 32.0 KiB, free 13.8 GiB)\n",
      "24/08/13 22:52:18 INFO MemoryStore: Block broadcast_48_piece0 stored as bytes in memory (estimated size 29.3 KiB, free 13.8 GiB)\n",
      "24/08/13 22:52:18 INFO BlockManagerInfo: Added broadcast_48_piece0 in memory on fa3024105736:40255 (size: 29.3 KiB, free: 13.8 GiB)\n",
      "24/08/13 22:52:18 INFO SparkContext: Created broadcast 48 from broadcast at SparkBatch.java:79\n",
      "24/08/13 22:52:18 INFO CodeGenerator: Code generated in 11.600399 ms\n",
      "24/08/13 22:52:18 INFO SparkContext: Starting job: show at cmd24.sc:1\n",
      "24/08/13 22:52:18 INFO DAGScheduler: Got job 16 (show at cmd24.sc:1) with 1 output partitions\n",
      "24/08/13 22:52:18 INFO DAGScheduler: Final stage: ResultStage 18 (show at cmd24.sc:1)\n",
      "24/08/13 22:52:18 INFO DAGScheduler: Parents of final stage: List()\n",
      "24/08/13 22:52:18 INFO DAGScheduler: Missing parents: List()\n",
      "24/08/13 22:52:18 INFO DAGScheduler: Submitting ResultStage 18 (MapPartitionsRDD[68] at show at cmd24.sc:1), which has no missing parents\n",
      "24/08/13 22:52:18 INFO MemoryStore: Block broadcast_49 stored as values in memory (estimated size 29.9 KiB, free 13.8 GiB)\n",
      "24/08/13 22:52:18 INFO MemoryStore: Block broadcast_49_piece0 stored as bytes in memory (estimated size 7.8 KiB, free 13.8 GiB)\n",
      "24/08/13 22:52:18 INFO BlockManagerInfo: Added broadcast_49_piece0 in memory on fa3024105736:40255 (size: 7.8 KiB, free: 13.8 GiB)\n",
      "24/08/13 22:52:18 INFO SparkContext: Created broadcast 49 from broadcast at DAGScheduler.scala:1585\n",
      "24/08/13 22:52:18 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 18 (MapPartitionsRDD[68] at show at cmd24.sc:1) (first 15 tasks are for partitions Vector(0))\n",
      "24/08/13 22:52:18 INFO TaskSchedulerImpl: Adding task set 18.0 with 1 tasks resource profile 0\n",
      "24/08/13 22:52:18 INFO TaskSetManager: Starting task 0.0 in stage 18.0 (TID 16) (fa3024105736, executor driver, partition 0, PROCESS_LOCAL, 16988 bytes) \n",
      "24/08/13 22:52:18 INFO Executor: Running task 0.0 in stage 18.0 (TID 16)\n",
      "24/08/13 22:52:18 INFO CodeGenerator: Code generated in 10.961618 ms\n",
      "24/08/13 22:52:18 INFO CodecPool: Got brand-new decompressor [.zstd]\n",
      "24/08/13 22:52:18 INFO Executor: Finished task 0.0 in stage 18.0 (TID 16). 10631 bytes result sent to driver\n",
      "24/08/13 22:52:18 INFO TaskSetManager: Finished task 0.0 in stage 18.0 (TID 16) in 34 ms on fa3024105736 (executor driver) (1/1)\n",
      "24/08/13 22:52:18 INFO TaskSchedulerImpl: Removed TaskSet 18.0, whose tasks have all completed, from pool \n",
      "24/08/13 22:52:18 INFO DAGScheduler: ResultStage 18 (show at cmd24.sc:1) finished in 0.039 s\n",
      "24/08/13 22:52:18 INFO DAGScheduler: Job 16 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "24/08/13 22:52:18 INFO TaskSchedulerImpl: Killing all running tasks in stage 18: Stage finished\n",
      "24/08/13 22:52:18 INFO DAGScheduler: Job 16 finished: show at cmd24.sc:1, took 0.041798 s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----------+--------------------+--------------------+-------------+--------+---------------------+-----------------------+--------------------+----------------------+----------------+------------------+-----------------+-------------------+-----------------------+-------------------------+-----------------------+-------------------------+--------------------+-----------------+--------------------+--------------------+--------------------+--------------------+-------------------------+-------------------+-------------------+\n",
      "|        EmployerName|EmployerId|             Address|            PostCode|CompanyNumber|SicCodes|DiffMeanHourlyPercent|DiffMedianHourlyPercent|DiffMeanBonusPercent|DiffMedianBonusPercent|MaleBonusPercent|FemaleBonusPercent|MaleLowerQuartile|FemaleLowerQuartile|MaleLowerMiddleQuartile|FemaleLowerMiddleQuartile|MaleUpperMiddleQuartile|FemaleUpperMiddleQuartile|     MaleTopQuartile|FemaleTopQuartile|CompanyLinkToGPGInfo|   ResponsiblePerson|        EmployerSize|         CurrentName|SubmittedAfterTheDeadline|            DueDate|      DateSubmitted|\n",
      "+--------------------+----------+--------------------+--------------------+-------------+--------+---------------------+-----------------------+--------------------+----------------------+----------------+------------------+-----------------+-------------------+-----------------------+-------------------------+-----------------------+-------------------------+--------------------+-----------------+--------------------+--------------------+--------------------+--------------------+-------------------------+-------------------+-------------------+\n",
      "|'PRIFYSGOL ABERYS...|     19070|Aberystwyth Unive...|            SY23 3FL|     RC000641|    NULL|                  8.5|                    3.7|                NULL|                  NULL|             0.0|               0.0|             51.7|               48.3|                   40.4|                     59.6|                   40.9|                     59.1|                55.6|             44.4|https://www.aber....|Dylan Jones (Dive...|        1000 to 4999|'PRIFYSGOL ABERYS...|                    False|2022/04/05 00:00:00|2022/03/31 13:09:40|\n",
      "|\"\"\"RED BAND\"\" CHE...|  LIMITED\"|               16879|19 Smith's Place,...|      EH6 8NU|SC016876|                47730|                    0.6|                -4.4|                  14.1|            -2.0|              21.8|             78.2|               41.0|                   59.0|                     23.2|                   76.8|                      4.9|                95.1|             20.5|                79.5|                NULL|Philip Galt (Mana...|          250 to 499|     \"\"\"RED BAND\"\" CHE...|           LIMITED\"|              False|\n",
      "|          1509 GROUP|     15320|Royal Grammar Sch...|             GU1 3BB|     04104101|  85200,|                 NULL|                   NULL|                NULL|                  NULL|            NULL|              NULL|             NULL|               NULL|                   NULL|                     NULL|                   NULL|                     NULL|                NULL|             NULL|                NULL|                NULL|                NULL|                NULL|                     NULL|               NULL|               NULL|\n",
      "|              85310\"|      18.0|                16.0|                NULL|         NULL|     0.0|                  0.0|                   36.0|                64.0|                  36.0|            64.0|              50.0|             50.0|               71.0|                   29.0|     https://www.rgsg....|   Ann Mortimer (Pay...|            Less than 250|          1509 GROUP|            False| 2022/04/05 00:00:00| 2022/02/02 11:06:02|                NULL|                NULL|                     NULL|               NULL|               NULL|\n",
      "|1LIFE MANAGEMENT ...|       687|The Stables, Duxb...|             PR7 4AT|     02566586|  93110,|                 NULL|                   NULL|                NULL|                  NULL|            NULL|              NULL|             NULL|               NULL|                   NULL|                     NULL|                   NULL|                     NULL|                NULL|             NULL|                NULL|                NULL|                NULL|                NULL|                     NULL|               NULL|               NULL|\n",
      "|               93130|      NULL|                NULL|                NULL|         NULL|    NULL|                 NULL|                   NULL|                NULL|                  NULL|            NULL|              NULL|             NULL|               NULL|                   NULL|                     NULL|                   NULL|                     NULL|                NULL|             NULL|                NULL|                NULL|                NULL|                NULL|                     NULL|               NULL|               NULL|\n",
      "|              93290\"|       6.1|               -35.3|                55.5|       -100.0|    69.2|                 30.8|                   58.3|                41.7|                  34.8|            65.2|              34.8|             65.2|               47.8|                   52.2|     https://www.1life...|   Ann Chesher (Head...|            Less than 250|1LIFE MANAGEMENT ...|            False| 2022/04/05 00:00:00| 2022/02/01 12:19:55|                NULL|                NULL|                     NULL|               NULL|               NULL|\n",
      "|  1ST HOME CARE LTD.|     17484|Suite 1, Ground F...|             G32 9AT|     SC272838|  86900,|                 NULL|                   NULL|                NULL|                  NULL|            NULL|              NULL|             NULL|               NULL|                   NULL|                     NULL|                   NULL|                     NULL|                NULL|             NULL|                NULL|                NULL|                NULL|                NULL|                     NULL|               NULL|               NULL|\n",
      "|              88100\"|      -3.0|                 0.0|                NULL|         NULL|     0.0|                  0.0|                    9.0|                91.0|                   9.0|            91.0|               9.0|             91.0|                8.0|                   92.0|                     NULL|   David Sargent (Gr...|            Less than 250|  1ST HOME CARE LTD.|            False| 2022/04/05 00:00:00| 2022/02/08 14:48:58|                NULL|                NULL|                     NULL|               NULL|               NULL|\n",
      "|2 AGRICULTURE LIM...|     14399|Fairview Mill, In...|            EH28 8NB|     SC156515|   1470,|                 NULL|                   NULL|                NULL|                  NULL|            NULL|              NULL|             NULL|               NULL|                   NULL|                     NULL|                   NULL|                     NULL|                NULL|             NULL|                NULL|                NULL|                NULL|                NULL|                     NULL|               NULL|               NULL|\n",
      "|              10910\"|        12|                16.9|               -20.5|        -69.3|    86.5|                   88|                   81.7|                18.3|                  95.8|             4.2|              95.8|              4.2|               91.7|                    8.3|                     NULL|   Kevin Sketcher (M...|               250 to 499|2 AGRICULTURE LIM...|            False| 2022/04/05 00:00:00| 2022/04/01 16:48:57|                NULL|                NULL|                     NULL|               NULL|               NULL|\n",
      "|2 SISTERS FOOD GR...|        77|Trinity Park Hous...|             WF2 8EE|     02826929|   10120|                  8.3|                    5.6|                32.1|                   2.7|             2.0|               2.0|             54.0|               46.0|                   57.0|                     43.0|                   66.0|                     34.0|                66.0|             34.0|https://www.2sfg....|Lee Greenbury (Di...|      5000 to 19,999|2 SISTERS FOOD GR...|                    False|2022/04/05 00:00:00|2022/02/11 12:39:54|\n",
      "|2 TOUCH BPO SERVI...|       690|4 Admiral Way, Do...|             SR3 3XW|     08942815|   82200|                  3.5|                    0.0|                -8.0|                 -69.0|            96.2|              93.2|             37.1|               62.9|                   33.8|                     66.2|                   39.1|                     60.9|                47.7|             52.3|https://www.weare...|Julie McIntosh (C...|          250 to 499|2 TOUCH BPO SERVI...|                    False|2022/04/05 00:00:00|2021/10/13 08:59:27|\n",
      "|23.5 DEGREES LIMITED|       694|Unit 3 Hedge End ...|            SO30 4RT|     08014079|   56103|                  7.9|                    0.0|                87.0|                   9.0|            11.0|              16.0|             28.0|               72.0|                   33.0|                     67.0|                   31.0|                     69.0|                34.0|             66.0|https://23-5degre...|Sarah Rawson (HR ...|        1000 to 4999|23.5 DEGREES LIMITED|                    False|2022/04/05 00:00:00|2021/11/08 14:40:30|\n",
      "|         24 X 7 LTD.|       695|Little Easton Man...|             CM6 2JN|     04142000|   52290|                  2.0|                    0.0|                 0.0|                   0.0|             1.2|               0.6|             50.0|               50.0|                   50.0|                     50.0|                   50.0|                     50.0|                54.0|             46.0|                NULL|      lisa hyem (fd)|        1000 to 4999|         24 X 7 LTD.|                    False|2022/04/05 00:00:00|2021/10/04 18:47:05|\n",
      "|24-7 EMPLOYMENT S...|     17921|198 Parrock Stree...|            DA12 1EW|     07431144|   78200|                    0|                    0.0|                NULL|                  NULL|             0.0|               0.0|             25.0|               75.0|                   44.0|                       56|                     47|                       53|                  31|               69|https://247esl.co...|Riju Sharma Adhik...|       Less than 250|24-7 EMPLOYMENT S...|                    False|2022/04/05 00:00:00|2022/03/29 15:55:27|\n",
      "|2GETHER SUPPORT S...|     19676|Management Office...|            TN24 0LZ|     11385580|   81100|                  2.2|                    0.0|                58.0|                  58.0|             0.2|               0.3|             38.4|               61.6|                   55.8|                     44.2|                   40.4|                     59.6|                59.7|             40.3|https://www.2geth...|Jacqui Siggers (P...|        1000 to 4999|2GETHER SUPPORT S...|                    False|2022/04/05 00:00:00|2022/03/07 13:06:06|\n",
      "|3 WAY CLEANING LI...|     17503|Unit 7 Capstan Ce...|            RM18 7HH|     03457928|   81210|                  3.0|                   -3.3|                NULL|                  NULL|             0.0|               0.6|             37.9|               62.1|                   50.8|                     49.2|                   37.3|                     62.7|                45.8|             54.2|                NULL|Sue Hawkins (Fina...|          250 to 499|3 WAY CLEANING LI...|                    False|2022/04/05 00:00:00|2021/07/01 14:45:20|\n",
      "|3663 TRANSPORT LI...|       697|814 Leigh Road, S...|             SL1 4BD|     00456846|   52290|                 -5.8|                    2.8|               -11.3|                  55.6|            36.4|              23.6|             93.5|                6.5|                   96.7|                      3.3|                   97.2|                      2.8|                93.5|              6.5|https://www.bidfo...|Heather Angus (Pe...|        1000 to 4999|3663 TRANSPORT LI...|                    False|2022/04/05 00:00:00|2022/04/03 17:13:45|\n",
      "|  3D LEISURE LIMITED|     19343|Peel House, Upper...|             GU9 7JN|     02538054|  82990,|                 NULL|                   NULL|                NULL|                  NULL|            NULL|              NULL|             NULL|               NULL|                   NULL|                     NULL|                   NULL|                     NULL|                NULL|             NULL|                NULL|                NULL|                NULL|                NULL|                     NULL|               NULL|               NULL|\n",
      "+--------------------+----------+--------------------+--------------------+-------------+--------+---------------------+-----------------------+--------------------+----------------------+----------------+------------------+-----------------+-------------------+-----------------------+-------------------------+-----------------------+-------------------------+--------------------+-----------------+--------------------+--------------------+--------------------+--------------------+-------------------------+-------------------+-------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "// Or the same with option + DF syntax\n",
    "spark.read.option(\"snapshot-id\", f\"${snapshot}\").table(\"local.uk_gender_pay_data\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "6f53692b-f14a-4df7-8069-147eca8da0cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mres27\u001b[39m: \u001b[32mDataFrame\u001b[39m = []"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"DROP TABLE IF EXISTS local.uk_gender_pay_data_postcode\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "8deb38c3-1e64-4eba-ac80-c75d5674258b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/08/13 22:55:33 INFO BaseMetastoreCatalog: Table properties set at catalog level through catalog properties: {}\n",
      "24/08/13 22:55:33 INFO BaseMetastoreCatalog: Table properties enforced at catalog level through catalog properties: {}\n",
      "24/08/13 22:55:33 INFO SparkWrite: Requesting 402653184 bytes advisory partition size for table uk_gender_pay_data_postcode\n",
      "24/08/13 22:55:33 INFO SparkWrite: Requesting ClusteredDistribution(truncate(1, PostCode)) as write distribution for table uk_gender_pay_data_postcode\n",
      "24/08/13 22:55:33 INFO SparkWrite: Requesting [] as write ordering for table uk_gender_pay_data_postcode\n",
      "24/08/13 22:55:33 INFO FileSourceStrategy: Pushed Filters: \n",
      "24/08/13 22:55:33 INFO FileSourceStrategy: Post-Scan Filters: \n",
      "24/08/13 22:55:33 INFO MemoryStore: Block broadcast_50 stored as values in memory (estimated size 199.4 KiB, free 13.8 GiB)\n",
      "24/08/13 22:55:33 INFO MemoryStore: Block broadcast_50_piece0 stored as bytes in memory (estimated size 34.4 KiB, free 13.8 GiB)\n",
      "24/08/13 22:55:33 INFO BlockManagerInfo: Added broadcast_50_piece0 in memory on fa3024105736:40255 (size: 34.4 KiB, free: 13.8 GiB)\n",
      "24/08/13 22:55:33 INFO SparkContext: Created broadcast 50 from sql at cmd28.sc:2\n",
      "24/08/13 22:55:33 INFO FileSourceScanExec: Planning scan with bin packing, max size: 8376950 bytes, open cost is considered as scanning 4194304 bytes.\n",
      "24/08/13 22:55:33 INFO DAGScheduler: Registering RDD 71 (sql at cmd28.sc:2) as input to shuffle 2\n",
      "24/08/13 22:55:33 INFO DAGScheduler: Got map stage job 17 (sql at cmd28.sc:2) with 1 output partitions\n",
      "24/08/13 22:55:33 INFO DAGScheduler: Final stage: ShuffleMapStage 19 (sql at cmd28.sc:2)\n",
      "24/08/13 22:55:33 INFO DAGScheduler: Parents of final stage: List()\n",
      "24/08/13 22:55:33 INFO DAGScheduler: Missing parents: List()\n",
      "24/08/13 22:55:33 INFO DAGScheduler: Submitting ShuffleMapStage 19 (MapPartitionsRDD[71] at sql at cmd28.sc:2), which has no missing parents\n",
      "24/08/13 22:55:33 INFO MemoryStore: Block broadcast_51 stored as values in memory (estimated size 20.4 KiB, free 13.8 GiB)\n",
      "24/08/13 22:55:33 INFO MemoryStore: Block broadcast_51_piece0 stored as bytes in memory (estimated size 9.5 KiB, free 13.8 GiB)\n",
      "24/08/13 22:55:33 INFO BlockManagerInfo: Added broadcast_51_piece0 in memory on fa3024105736:40255 (size: 9.5 KiB, free: 13.8 GiB)\n",
      "24/08/13 22:55:33 INFO SparkContext: Created broadcast 51 from broadcast at DAGScheduler.scala:1585\n",
      "24/08/13 22:55:33 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 19 (MapPartitionsRDD[71] at sql at cmd28.sc:2) (first 15 tasks are for partitions Vector(0))\n",
      "24/08/13 22:55:33 INFO TaskSchedulerImpl: Adding task set 19.0 with 1 tasks resource profile 0\n",
      "24/08/13 22:55:33 INFO TaskSetManager: Starting task 0.0 in stage 19.0 (TID 17) (fa3024105736, executor driver, partition 0, PROCESS_LOCAL, 9688 bytes) \n",
      "24/08/13 22:55:33 INFO Executor: Running task 0.0 in stage 19.0 (TID 17)\n",
      "24/08/13 22:55:33 INFO CodeGenerator: Code generated in 18.955493 ms\n",
      "24/08/13 22:55:33 INFO FileScanRDD: Reading File path: file:///high-performance-spark-examples/data/fetched/2021, range: 0-4182646, partition values: [empty row]\n",
      "24/08/13 22:55:33 INFO Executor: Finished task 0.0 in stage 19.0 (TID 17). 2006 bytes result sent to driver\n",
      "24/08/13 22:55:33 INFO TaskSetManager: Finished task 0.0 in stage 19.0 (TID 17) in 445 ms on fa3024105736 (executor driver) (1/1)\n",
      "24/08/13 22:55:33 INFO TaskSchedulerImpl: Removed TaskSet 19.0, whose tasks have all completed, from pool \n",
      "24/08/13 22:55:33 INFO DAGScheduler: ShuffleMapStage 19 (sql at cmd28.sc:2) finished in 0.465 s\n",
      "24/08/13 22:55:33 INFO DAGScheduler: looking for newly runnable stages\n",
      "24/08/13 22:55:33 INFO DAGScheduler: running: HashSet()\n",
      "24/08/13 22:55:33 INFO DAGScheduler: waiting: HashSet()\n",
      "24/08/13 22:55:33 INFO DAGScheduler: failed: HashSet()\n",
      "24/08/13 22:55:33 INFO ShufflePartitionsUtil: For shuffle(2), advisory target size: 402653184, actual target size 3405720, minimum partition size: 1048576\n",
      "24/08/13 22:55:33 INFO MemoryStore: Block broadcast_52 stored as values in memory (estimated size 32.0 KiB, free 13.8 GiB)\n",
      "24/08/13 22:55:33 INFO MemoryStore: Block broadcast_52_piece0 stored as bytes in memory (estimated size 29.2 KiB, free 13.8 GiB)\n",
      "24/08/13 22:55:33 INFO BlockManagerInfo: Added broadcast_52_piece0 in memory on fa3024105736:40255 (size: 29.2 KiB, free: 13.8 GiB)\n",
      "24/08/13 22:55:33 INFO SparkContext: Created broadcast 52 from broadcast at SparkWrite.java:193\n",
      "24/08/13 22:55:33 INFO AppendDataExec: Start processing data source write support: IcebergBatchWrite(table=uk_gender_pay_data_postcode, format=PARQUET). The input RDD has 1 partitions.\n",
      "24/08/13 22:55:33 INFO SparkContext: Starting job: sql at cmd28.sc:2\n",
      "24/08/13 22:55:33 INFO DAGScheduler: Got job 18 (sql at cmd28.sc:2) with 1 output partitions\n",
      "24/08/13 22:55:33 INFO DAGScheduler: Final stage: ResultStage 21 (sql at cmd28.sc:2)\n",
      "24/08/13 22:55:33 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 20)\n",
      "24/08/13 22:55:33 INFO DAGScheduler: Missing parents: List()\n",
      "24/08/13 22:55:33 INFO DAGScheduler: Submitting ResultStage 21 (ShuffledRowRDD[72] at sql at cmd28.sc:2), which has no missing parents\n",
      "24/08/13 22:55:33 INFO MemoryStore: Block broadcast_53 stored as values in memory (estimated size 12.1 KiB, free 13.8 GiB)\n",
      "24/08/13 22:55:33 INFO MemoryStore: Block broadcast_53_piece0 stored as bytes in memory (estimated size 6.1 KiB, free 13.8 GiB)\n",
      "24/08/13 22:55:33 INFO BlockManagerInfo: Added broadcast_53_piece0 in memory on fa3024105736:40255 (size: 6.1 KiB, free: 13.8 GiB)\n",
      "24/08/13 22:55:33 INFO SparkContext: Created broadcast 53 from broadcast at DAGScheduler.scala:1585\n",
      "24/08/13 22:55:33 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 21 (ShuffledRowRDD[72] at sql at cmd28.sc:2) (first 15 tasks are for partitions Vector(0))\n",
      "24/08/13 22:55:33 INFO TaskSchedulerImpl: Adding task set 21.0 with 1 tasks resource profile 0\n",
      "24/08/13 22:55:33 INFO TaskSetManager: Starting task 0.0 in stage 21.0 (TID 18) (fa3024105736, executor driver, partition 0, NODE_LOCAL, 9079 bytes) \n",
      "24/08/13 22:55:33 INFO Executor: Running task 0.0 in stage 21.0 (TID 18)\n",
      "24/08/13 22:55:33 INFO ShuffleBlockFetcherIterator: Getting 1 (3.2 MiB) non-empty blocks including 1 (3.2 MiB) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
      "24/08/13 22:55:33 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms\n",
      "24/08/13 22:55:33 INFO CodecPool: Got brand-new compressor [.zstd]\n",
      "24/08/13 22:55:33 INFO CodecPool: Got brand-new compressor [.zstd]\n",
      "24/08/13 22:55:33 INFO CodecPool: Got brand-new compressor [.zstd]\n",
      "24/08/13 22:55:33 INFO CodecPool: Got brand-new compressor [.zstd]\n",
      "24/08/13 22:55:33 INFO CodecPool: Got brand-new compressor [.zstd]\n",
      "24/08/13 22:55:33 INFO CodecPool: Got brand-new compressor [.zstd]\n",
      "24/08/13 22:55:33 INFO CodecPool: Got brand-new compressor [.zstd]\n",
      "24/08/13 22:55:33 INFO CodecPool: Got brand-new compressor [.zstd]\n",
      "24/08/13 22:55:33 INFO CodecPool: Got brand-new compressor [.zstd]\n",
      "24/08/13 22:55:33 INFO CodecPool: Got brand-new compressor [.zstd]\n",
      "24/08/13 22:55:33 INFO CodecPool: Got brand-new compressor [.zstd]\n",
      "24/08/13 22:55:33 INFO CodecPool: Got brand-new compressor [.zstd]\n",
      "24/08/13 22:55:33 INFO CodecPool: Got brand-new compressor [.zstd]\n",
      "24/08/13 22:55:33 INFO CodecPool: Got brand-new compressor [.zstd]\n",
      "24/08/13 22:55:33 INFO CodecPool: Got brand-new compressor [.zstd]\n",
      "24/08/13 22:55:33 INFO CodecPool: Got brand-new compressor [.zstd]\n",
      "24/08/13 22:55:33 INFO CodecPool: Got brand-new compressor [.zstd]\n",
      "24/08/13 22:55:33 INFO CodecPool: Got brand-new compressor [.zstd]\n",
      "24/08/13 22:55:33 INFO CodecPool: Got brand-new compressor [.zstd]\n",
      "24/08/13 22:55:33 INFO CodecPool: Got brand-new compressor [.zstd]\n",
      "24/08/13 22:55:33 INFO CodecPool: Got brand-new compressor [.zstd]\n",
      "24/08/13 22:55:33 INFO CodecPool: Got brand-new compressor [.zstd]\n",
      "24/08/13 22:55:33 INFO CodecPool: Got brand-new compressor [.zstd]\n",
      "24/08/13 22:55:33 INFO CodecPool: Got brand-new compressor [.zstd]\n",
      "24/08/13 22:55:33 INFO CodecPool: Got brand-new compressor [.zstd]\n",
      "24/08/13 22:55:33 INFO CodecPool: Got brand-new compressor [.zstd]\n",
      "24/08/13 22:55:33 INFO CodecPool: Got brand-new compressor [.zstd]\n",
      "24/08/13 22:55:33 INFO CodecPool: Got brand-new compressor [.zstd]\n",
      "24/08/13 22:55:33 INFO CodecPool: Got brand-new compressor [.zstd]\n",
      "24/08/13 22:55:33 INFO CodecPool: Got brand-new compressor [.zstd]\n",
      "24/08/13 22:55:33 INFO CodecPool: Got brand-new compressor [.zstd]\n",
      "24/08/13 22:55:33 INFO CodecPool: Got brand-new compressor [.zstd]\n",
      "24/08/13 22:55:33 INFO CodecPool: Got brand-new compressor [.zstd]\n",
      "24/08/13 22:55:33 INFO CodecPool: Got brand-new compressor [.zstd]\n",
      "24/08/13 22:55:33 INFO CodecPool: Got brand-new compressor [.zstd]\n",
      "24/08/13 22:55:33 INFO DataWritingSparkTask: Writer for partition 0 is committing.\n",
      "24/08/13 22:55:34 INFO DataWritingSparkTask: Committed partition 0 (task 18, attempt 0, stage 21.0)\n",
      "24/08/13 22:55:34 INFO Executor: Finished task 0.0 in stage 21.0 (TID 18). 116005 bytes result sent to driver\n",
      "24/08/13 22:55:34 INFO TaskSetManager: Finished task 0.0 in stage 21.0 (TID 18) in 1119 ms on fa3024105736 (executor driver) (1/1)\n",
      "24/08/13 22:55:34 INFO TaskSchedulerImpl: Removed TaskSet 21.0, whose tasks have all completed, from pool \n",
      "24/08/13 22:55:34 INFO DAGScheduler: ResultStage 21 (sql at cmd28.sc:2) finished in 1.130 s\n",
      "24/08/13 22:55:34 INFO DAGScheduler: Job 18 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "24/08/13 22:55:34 INFO TaskSchedulerImpl: Killing all running tasks in stage 21: Stage finished\n",
      "24/08/13 22:55:34 INFO DAGScheduler: Job 18 finished: sql at cmd28.sc:2, took 1.133560 s\n",
      "24/08/13 22:55:34 INFO AppendDataExec: Data source write support IcebergBatchWrite(table=uk_gender_pay_data_postcode, format=PARQUET) is committing.\n",
      "24/08/13 22:55:34 INFO SparkWrite: Committing append with 35 new data files to table uk_gender_pay_data_postcode\n",
      "24/08/13 22:55:34 INFO SnapshotProducer: Committed snapshot 446658414211481299 (MergeAppend)\n",
      "24/08/13 22:55:34 INFO LoggingMetricsReporter: Received metrics report: CommitReport{tableName=uk_gender_pay_data_postcode, snapshotId=446658414211481299, sequenceNumber=1, operation=append, commitMetrics=CommitMetricsResult{totalDuration=TimerResult{timeUnit=NANOSECONDS, totalDuration=PT0.055694566S, count=1}, attempts=CounterResult{unit=COUNT, value=1}, addedDataFiles=CounterResult{unit=COUNT, value=35}, removedDataFiles=null, totalDataFiles=CounterResult{unit=COUNT, value=35}, addedDeleteFiles=null, addedEqualityDeleteFiles=null, addedPositionalDeleteFiles=null, removedDeleteFiles=null, removedEqualityDeleteFiles=null, removedPositionalDeleteFiles=null, totalDeleteFiles=CounterResult{unit=COUNT, value=0}, addedRecords=CounterResult{unit=COUNT, value=15395}, removedRecords=null, totalRecords=CounterResult{unit=COUNT, value=15395}, addedFilesSizeInBytes=CounterResult{unit=BYTES, value=1798429}, removedFilesSizeInBytes=null, totalFilesSizeInBytes=CounterResult{unit=BYTES, value=1798429}, addedPositionalDeletes=null, removedPositionalDeletes=null, totalPositionalDeletes=CounterResult{unit=COUNT, value=0}, addedEqualityDeletes=null, removedEqualityDeletes=null, totalEqualityDeletes=CounterResult{unit=COUNT, value=0}}, metadata={engine-version=3.5.2, app-id=local-1723589343398, engine-name=spark, iceberg-version=Apache Iceberg 1.6.0 (commit 229d8f6fcd109e6c8943ea7cbb41dab746c6d0ed)}}\n",
      "24/08/13 22:55:34 INFO SparkWrite: Committed in 56 ms\n",
      "24/08/13 22:55:34 INFO AppendDataExec: Data source write support IcebergBatchWrite(table=uk_gender_pay_data_postcode, format=PARQUET) committed.\n",
      "24/08/13 22:55:34 INFO HadoopTableOperations: Committed a new metadata file /high-performance-spark-examples/warehouse/uk_gender_pay_data_postcode/metadata/v1.metadata.json\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[36mres28_1\u001b[39m: \u001b[32mDataFrame\u001b[39m = []"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Write the data out\n",
    "df.registerTempTable(\"temp_table\")\n",
    "spark.sql(\"CREATE TABLE local.uk_gender_pay_data_postcode USING iceberg PARTITIONED BY (truncate(1, PostCode)) AS select * from temp_table\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e87e6b08-5c0e-4356-a0ee-7245b7d7790b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/08/13 22:57:09 INFO BaseMetastoreCatalog: Table loaded by catalog: local.uk_gender_pay_data_postcode\n",
      "24/08/13 22:57:09 INFO V2ScanRelationPushDown: \n",
      "Output: content#1818, file_path#1819, file_format#1820, spec_id#1821, partition#1822, record_count#1823L, file_size_in_bytes#1824L, column_sizes#1825, value_counts#1826, null_value_counts#1827, nan_value_counts#1828, lower_bounds#1829, upper_bounds#1830, key_metadata#1831, split_offsets#1832, equality_ids#1833, sort_order_id#1834, readable_metrics#1835\n",
      "         \n",
      "24/08/13 22:57:09 INFO SnapshotScan: Scanning table local.uk_gender_pay_data_postcode snapshot 446658414211481299 created at 2024-08-13T22:55:34.817+00:00 with filter true\n",
      "24/08/13 22:57:09 INFO SparkPartitioningAwareScan: Reporting UnknownPartitioning with 1 partition(s) for table local.uk_gender_pay_data_postcode.files\n",
      "24/08/13 22:57:09 INFO MemoryStore: Block broadcast_54 stored as values in memory (estimated size 32.0 KiB, free 13.8 GiB)\n",
      "24/08/13 22:57:09 INFO MemoryStore: Block broadcast_54_piece0 stored as bytes in memory (estimated size 32.5 KiB, free 13.8 GiB)\n",
      "24/08/13 22:57:09 INFO BlockManagerInfo: Added broadcast_54_piece0 in memory on fa3024105736:40255 (size: 32.5 KiB, free: 13.8 GiB)\n",
      "24/08/13 22:57:09 INFO SparkContext: Created broadcast 54 from broadcast at SparkBatch.java:79\n",
      "24/08/13 22:57:09 INFO MemoryStore: Block broadcast_55 stored as values in memory (estimated size 32.0 KiB, free 13.8 GiB)\n",
      "24/08/13 22:57:09 INFO MemoryStore: Block broadcast_55_piece0 stored as bytes in memory (estimated size 32.5 KiB, free 13.8 GiB)\n",
      "24/08/13 22:57:09 INFO BlockManagerInfo: Added broadcast_55_piece0 in memory on fa3024105736:40255 (size: 32.5 KiB, free: 13.8 GiB)\n",
      "24/08/13 22:57:09 INFO SparkContext: Created broadcast 55 from broadcast at SparkBatch.java:79\n",
      "24/08/13 22:57:09 INFO SparkContext: Starting job: show at cmd29.sc:1\n",
      "24/08/13 22:57:09 INFO DAGScheduler: Got job 19 (show at cmd29.sc:1) with 1 output partitions\n",
      "24/08/13 22:57:09 INFO DAGScheduler: Final stage: ResultStage 22 (show at cmd29.sc:1)\n",
      "24/08/13 22:57:09 INFO DAGScheduler: Parents of final stage: List()\n",
      "24/08/13 22:57:09 INFO DAGScheduler: Missing parents: List()\n",
      "24/08/13 22:57:09 INFO DAGScheduler: Submitting ResultStage 22 (MapPartitionsRDD[76] at show at cmd29.sc:1), which has no missing parents\n",
      "24/08/13 22:57:09 INFO MemoryStore: Block broadcast_56 stored as values in memory (estimated size 43.5 KiB, free 13.8 GiB)\n",
      "24/08/13 22:57:09 INFO MemoryStore: Block broadcast_56_piece0 stored as bytes in memory (estimated size 10.9 KiB, free 13.8 GiB)\n",
      "24/08/13 22:57:09 INFO BlockManagerInfo: Added broadcast_56_piece0 in memory on fa3024105736:40255 (size: 10.9 KiB, free: 13.8 GiB)\n",
      "24/08/13 22:57:09 INFO SparkContext: Created broadcast 56 from broadcast at DAGScheduler.scala:1585\n",
      "24/08/13 22:57:09 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 22 (MapPartitionsRDD[76] at show at cmd29.sc:1) (first 15 tasks are for partitions Vector(0))\n",
      "24/08/13 22:57:09 INFO TaskSchedulerImpl: Adding task set 22.0 with 1 tasks resource profile 0\n",
      "24/08/13 22:57:09 INFO TaskSetManager: Starting task 0.0 in stage 22.0 (TID 19) (fa3024105736, executor driver, partition 0, PROCESS_LOCAL, 126270 bytes) \n",
      "24/08/13 22:57:09 INFO Executor: Running task 0.0 in stage 22.0 (TID 19)\n",
      "24/08/13 22:57:09 INFO CodeGenerator: Code generated in 49.783073 ms\n",
      "24/08/13 22:57:10 INFO BlockManagerInfo: Removed broadcast_51_piece0 on fa3024105736:40255 in memory (size: 9.5 KiB, free: 13.8 GiB)\n",
      "24/08/13 22:57:10 INFO BlockManagerInfo: Removed broadcast_36_piece0 on fa3024105736:40255 in memory (size: 14.7 KiB, free: 13.8 GiB)\n",
      "24/08/13 22:57:10 INFO BlockManagerInfo: Removed broadcast_53_piece0 on fa3024105736:40255 in memory (size: 6.1 KiB, free: 13.8 GiB)\n",
      "24/08/13 22:57:10 INFO BlockManagerInfo: Removed broadcast_42_piece0 on fa3024105736:40255 in memory (size: 29.4 KiB, free: 13.8 GiB)\n",
      "24/08/13 22:57:10 INFO BlockManagerInfo: Removed broadcast_28_piece0 on fa3024105736:40255 in memory (size: 29.4 KiB, free: 13.8 GiB)\n",
      "24/08/13 22:57:10 INFO BlockManagerInfo: Removed broadcast_50_piece0 on fa3024105736:40255 in memory (size: 34.4 KiB, free: 13.8 GiB)\n",
      "24/08/13 22:57:10 INFO BlockManagerInfo: Removed broadcast_47_piece0 on fa3024105736:40255 in memory (size: 29.4 KiB, free: 13.8 GiB)\n",
      "24/08/13 22:57:10 INFO BlockManagerInfo: Removed broadcast_29_piece0 on fa3024105736:40255 in memory (size: 29.3 KiB, free: 13.8 GiB)\n",
      "24/08/13 22:57:10 INFO BlockManagerInfo: Removed broadcast_39_piece0 on fa3024105736:40255 in memory (size: 10.4 KiB, free: 13.8 GiB)\n",
      "24/08/13 22:57:10 INFO BlockManagerInfo: Removed broadcast_52_piece0 on fa3024105736:40255 in memory (size: 29.2 KiB, free: 13.8 GiB)\n",
      "24/08/13 22:57:10 INFO BlockManagerInfo: Removed broadcast_48_piece0 on fa3024105736:40255 in memory (size: 29.3 KiB, free: 13.8 GiB)\n",
      "24/08/13 22:57:10 INFO BlockManagerInfo: Removed broadcast_44_piece0 on fa3024105736:40255 in memory (size: 29.4 KiB, free: 13.8 GiB)\n",
      "24/08/13 22:57:10 INFO BlockManagerInfo: Removed broadcast_46_piece0 on fa3024105736:40255 in memory (size: 7.9 KiB, free: 13.8 GiB)\n",
      "24/08/13 22:57:10 INFO BlockManagerInfo: Removed broadcast_35_piece0 on fa3024105736:40255 in memory (size: 12.8 KiB, free: 13.8 GiB)\n",
      "24/08/13 22:57:10 INFO BlockManagerInfo: Removed broadcast_49_piece0 on fa3024105736:40255 in memory (size: 7.8 KiB, free: 13.8 GiB)\n",
      "24/08/13 22:57:10 INFO Executor: Finished task 0.0 in stage 22.0 (TID 19). 46025 bytes result sent to driver\n",
      "24/08/13 22:57:10 INFO TaskSetManager: Finished task 0.0 in stage 22.0 (TID 19) in 213 ms on fa3024105736 (executor driver) (1/1)\n",
      "24/08/13 22:57:10 INFO TaskSchedulerImpl: Removed TaskSet 22.0, whose tasks have all completed, from pool \n",
      "24/08/13 22:57:10 INFO DAGScheduler: ResultStage 22 (show at cmd29.sc:1) finished in 0.232 s\n",
      "24/08/13 22:57:10 INFO DAGScheduler: Job 19 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "24/08/13 22:57:10 INFO TaskSchedulerImpl: Killing all running tasks in stage 22: Stage finished\n",
      "24/08/13 22:57:10 INFO DAGScheduler: Job 19 finished: show at cmd29.sc:1, took 0.235696 s\n",
      "24/08/13 22:57:10 INFO BlockManagerInfo: Removed broadcast_40_piece0 on fa3024105736:40255 in memory (size: 29.3 KiB, free: 13.8 GiB)\n",
      "24/08/13 22:57:10 INFO BlockManagerInfo: Removed broadcast_45_piece0 on fa3024105736:40255 in memory (size: 29.3 KiB, free: 13.8 GiB)\n",
      "24/08/13 22:57:10 INFO BlockManagerInfo: Removed broadcast_43_piece0 on fa3024105736:40255 in memory (size: 29.3 KiB, free: 13.8 GiB)\n",
      "24/08/13 22:57:10 INFO BlockManagerInfo: Removed broadcast_41_piece0 on fa3024105736:40255 in memory (size: 9.0 KiB, free: 13.8 GiB)\n",
      "24/08/13 22:57:10 INFO CodeGenerator: Code generated in 12.904588 ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------------+-----------+-------+---------+------------+------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+------------+-------------+------------+-------------+--------------------+\n",
      "|content|           file_path|file_format|spec_id|partition|record_count|file_size_in_bytes|        column_sizes|        value_counts|   null_value_counts|    nan_value_counts|        lower_bounds|        upper_bounds|key_metadata|split_offsets|equality_ids|sort_order_id|    readable_metrics|\n",
      "+-------+--------------------+-----------+-------+---------+------------+------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+------------+-------------+------------+-------------+--------------------+\n",
      "|      0|/high-performance...|    PARQUET|      0|      {9}|          57|             15272|{1 -> 282, 2 -> 2...|{1 -> 57, 2 -> 57...|{1 -> 0, 2 -> 0, ...|{8 -> 0, 9 -> 0, ...|{1 -> [31 31 30 3...|{1 -> [39 33 31 3...|        NULL|          [4]|        NULL|            0|{{241, 57, 0, NUL...|\n",
      "|      0|/high-performance...|    PARQUET|      0|      {7}|         147|             23753|{1 -> 475, 2 -> 5...|{1 -> 147, 2 -> 1...|{1 -> 0, 2 -> 0, ...|{8 -> 0, 9 -> 0, ...|{1 -> [31 36 33 3...|{1 -> [39 37 30 3...|        NULL|          [4]|        NULL|            0|{{461, 147, 0, NU...|\n",
      "|      0|/high-performance...|    PARQUET|      0|      {8}|          95|             19647|{1 -> 405, 2 -> 3...|{1 -> 95, 2 -> 95...|{1 -> 0, 2 -> 0, ...|{8 -> 0, 9 -> 0, ...|{1 -> [31 31 30 3...|{1 -> [55 42 53 2...|        NULL|          [4]|        NULL|            0|{{395, 95, 0, NUL...|\n",
      "|      0|/high-performance...|    PARQUET|      0|      {A}|         129|             26151|{1 -> 1833, 2 -> ...|{1 -> 129, 2 -> 1...|{1 -> 0, 2 -> 0, ...|{8 -> 0, 9 -> 0, ...|{1 -> [41 42 42 4...|{1 -> [58 4F 44 5...|        NULL|          [4]|        NULL|            0|{{3200, 129, 0, N...|\n",
      "|      0|/high-performance...|    PARQUET|      0|      {B}|        1003|            116171|{1 -> 11838, 2 ->...|{1 -> 1003, 2 -> ...|{1 -> 0, 2 -> 0, ...|{8 -> 0, 9 -> 0, ...|{1 -> [41 20 26 2...|{1 -> [6B 69 6E 6...|        NULL|          [4]|        NULL|            0|{{22898, 1003, 0,...|\n",
      "|      0|/high-performance...|    PARQUET|      0|      {E}|        1209|            142492|{1 -> 14258, 2 ->...|{1 -> 1209, 2 -> ...|{1 -> 0, 2 -> 0, ...|{8 -> 0, 9 -> 0, ...|{1 -> [32 20 41 4...|{1 -> [68 61 79 7...|        NULL|          [4]|        NULL|            0|{{19309, 1209, 0,...|\n",
      "|      0|/high-performance...|    PARQUET|      0|      {F}|          54|             17642|{1 -> 1002, 2 -> ...|{1 -> 54, 2 -> 54...|{1 -> 0, 2 -> 0, ...|{8 -> 0, 9 -> 0, ...|{1 -> [41 42 45 5...|{1 -> [68 74 74 7...|        NULL|          [4]|        NULL|            0|{{1653, 54, 0, NU...|\n",
      "|      0|/high-performance...|    PARQUET|      0|      {C}|         759|             93088|{1 -> 9309, 2 -> ...|{1 -> 759, 2 -> 7...|{1 -> 0, 2 -> 0, ...|{8 -> 0, 9 -> 0, ...|{1 -> [32 34 20 5...|{1 -> [5A 4F 54 4...|        NULL|          [4]|        NULL|            0|{{18493, 759, 0, ...|\n",
      "|      0|/high-performance...|    PARQUET|      0|      {D}|         407|             56750|{1 -> 5373, 2 -> ...|{1 -> 407, 2 -> 4...|{1 -> 0, 2 -> 0, ...|{8 -> 0, 9 -> 0, ...|{1 -> [32 34 2D 3...|{1 -> [59 4F 55 5...|        NULL|          [4]|        NULL|            0|{{10518, 407, 0, ...|\n",
      "|      0|/high-performance...|    PARQUET|      0|      {I}|         137|             27143|{1 -> 2076, 2 -> ...|{1 -> 137, 2 -> 1...|{1 -> 0, 2 -> 0, ...|{8 -> 0, 9 -> 0, ...|{1 -> [41 43 43 4...|{1 -> [73 75 66 6...|        NULL|          [4]|        NULL|            0|{{3658, 137, 0, N...|\n",
      "|      0|/high-performance...|    PARQUET|      0|      {J}|           2|              9124|{1 -> 82, 2 -> 54...|{1 -> 2, 2 -> 2, ...|{1 -> 0, 2 -> 0, ...|{8 -> 0, 9 -> 0, ...|{1 -> [41 4D 42 4...|{1 -> [52 42 53 2...|        NULL|          [4]|        NULL|            0|{{140, 2, 0, NULL...|\n",
      "|      0|/high-performance...|    PARQUET|      0|      {G}|         375|             53813|{1 -> 4804, 2 -> ...|{1 -> 375, 2 -> 3...|{1 -> 0, 2 -> 0, ...|{8 -> 0, 9 -> 0, ...|{1 -> [31 35 30 3...|{1 -> [73 70 6F 7...|        NULL|          [4]|        NULL|            0|{{8494, 375, 0, N...|\n",
      "|      0|/high-performance...|    PARQUET|      0|      {H}|         291|             43421|{1 -> 3975, 2 -> ...|{1 -> 291, 2 -> 2...|{1 -> 0, 2 -> 0, ...|{8 -> 0, 9 -> 0, ...|{1 -> [38 58 38 2...|{1 -> [6D 69 63 7...|        NULL|          [4]|        NULL|            0|{{7199, 291, 0, N...|\n",
      "|      0|/high-performance...|    PARQUET|      0|      {M}|         465|             63484|{1 -> 5905, 2 -> ...|{1 -> 465, 2 -> 4...|{1 -> 0, 2 -> 0, ...|{8 -> 0, 9 -> 0, ...|{1 -> [35 20 44 4...|{1 -> [58 45 52 4...|        NULL|          [4]|        NULL|            0|{{10809, 465, 0, ...|\n",
      "|      0|/high-performance...|    PARQUET|      0|      {N}|         853|             99932|{1 -> 10142, 2 ->...|{1 -> 853, 2 -> 8...|{1 -> 0, 2 -> 0, ...|{8 -> 0, 9 -> 0, ...|{1 -> [41 20 26 2...|{1 -> [77 61 72 6...|        NULL|          [4]|        NULL|            0|{{18474, 853, 0, ...|\n",
      "|      0|/high-performance...|    PARQUET|      0|      {K}|         179|             30590|{1 -> 2648, 2 -> ...|{1 -> 179, 2 -> 1...|{1 -> 0, 2 -> 0, ...|{8 -> 0, 9 -> 0, ...|{1 -> [41 42 42 4...|{1 -> [73 74 20 6...|        NULL|          [4]|        NULL|            0|{{4289, 179, 0, N...|\n",
      "|      0|/high-performance...|    PARQUET|      0|      {L}|         609|             76471|{1 -> 7839, 2 -> ...|{1 -> 609, 2 -> 6...|{1 -> 0, 2 -> 0, ...|{8 -> 0, 9 -> 0, ...|{1 -> [41 42 41 4...|{1 -> [63 6F 6C 6...|        NULL|          [4]|        NULL|            0|{{13858, 609, 0, ...|\n",
      "|      0|/high-performance...|    PARQUET|      0|   {NULL}|        2776|            103060|{1 -> 5701, 2 -> ...|{1 -> 2776, 2 -> ...|{1 -> 0, 2 -> 157...|{8 -> 0, 9 -> 0, ...|{1 -> [31 30 31 3...|{1 -> [56 45 52 4...|        NULL|          [4]|        NULL|            0|{{4683, 2776, 159...|\n",
      "|      0|/high-performance...|    PARQUET|      0|      {R}|         369|             51814|{1 -> 4757, 2 -> ...|{1 -> 369, 2 -> 3...|{1 -> 0, 2 -> 0, ...|{8 -> 0, 9 -> 0, ...|{1 -> [33 20 57 4...|{1 -> [5A 49 50 4...|        NULL|          [4]|        NULL|            0|{{7905, 369, 0, N...|\n",
      "|      0|/high-performance...|    PARQUET|      0|      {O}|         218|             35791|{1 -> 3072, 2 -> ...|{1 -> 218, 2 -> 2...|{1 -> 0, 2 -> 0, ...|{8 -> 0, 9 -> 0, ...|{1 -> [41 2D 50 4...|{1 -> [5A 45 4E 2...|        NULL|          [4]|        NULL|            0|{{4909, 218, 0, N...|\n",
      "+-------+--------------------+-----------+-------+---------+------------+------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+------------+-------------+------------+-------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT * FROM local.uk_gender_pay_data_postcode.files\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "921d0c02-1fb7-43ec-ac0a-b5d1c3a40c3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "// Huzzah!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "3b8ff0a3-8c6e-4d67-8afb-d1541c7e6dbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "+ python python_check.py\n",
      "/high-performance-spark-examples/spark-upgrade/e2e_demo/scala/run_demo.sh: line 7: python: command not found\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31mjava.lang.RuntimeException: Nonzero exit value: 127\u001b[39m\n  scala.sys.process.ProcessBuilderImpl$AbstractBuilder.slurp(\u001b[32mProcessBuilderImpl.scala\u001b[39m:\u001b[32m164\u001b[39m)\n  scala.sys.process.ProcessBuilderImpl$AbstractBuilder.$bang$bang(\u001b[32mProcessBuilderImpl.scala\u001b[39m:\u001b[32m121\u001b[39m)\n  ammonite.$sess.cmd30$Helper.<init>(\u001b[32mcmd30.sc\u001b[39m:\u001b[32m1\u001b[39m)\n  ammonite.$sess.cmd30$.<clinit>(\u001b[32mcmd30.sc\u001b[39m:\u001b[32m7\u001b[39m)"
     ]
    }
   ],
   "source": [
    "// Lets go take a look at a quick side-by-side test\n",
    "\"/high-performance-spark-examples/spark-upgrade/e2e_demo/scala/run_demo.sh\".!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05d47a57-3bfa-484a-90ed-0231a17a7205",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Scala 2.13 (w/ Spark 3.5 & Iceberg 1.6)",
   "language": "scala",
   "name": "scala2.13"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".sc",
   "mimetype": "text/x-scala",
   "name": "scala",
   "nbconvert_exporter": "script",
   "version": "2.13.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
