{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34577ad3-822f-4370-bcba-56b9fcec3196",
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.sql._\n",
    "import scala.sys.process._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d3141ec-7779-467a-9f76-2e51030fd1c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "// So now we need to configure Spark to use Iceberg\n",
    "// See https://iceberg.apache.org/docs/1.6.0/spark-configuration/ & https://iceberg.apache.org/docs/1.6.0/spark-getting-started/\n",
    "// We'll use the \"hadoop\" (aka file) catalog & /high-performance-spark-examples/warehouse for the location\n",
    "val spark = (SparkSession.builder.master(\"local[*]\")\n",
    "             // Setup the extensions\n",
    "             // You'll want to configure Iceberg here as discussed above\n",
    "             // If you want to match the solution you'll want to configure the Iceberg catalog to be \"local.\"\n",
    "             .getOrCreate()\n",
    "             )\n",
    "import spark._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c53080aa-a9d6-45f9-968b-8e052e7fa963",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88d54d05-0c49-4268-9b65-8c72679cb0f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sparkContext.uiWebUrl.get"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "270730c9-9787-407c-ba22-f0cee1f67f53",
   "metadata": {},
   "outputs": [],
   "source": [
    "// Load the current data\n",
    "val df = spark.read.option(\"header\", \"true\").option(\"inferSchema\", \"true\").csv(\"/high-performance-spark-examples/data/fetched/2021\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87ca6359-86bc-42a4-93dd-4fc64496b145",
   "metadata": {},
   "outputs": [],
   "source": [
    "// Drop existing table if present & create new table\n",
    "spark.sql(\"DROP TABLE IF EXISTS local.uk_gender_pay_data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bdeb3eb-b725-409b-ab3a-409d0e8309ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "// Write the data out\n",
    "df.write.saveAsTable(\"local.uk_gender_pay_data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "554c6036-0c6b-4e3c-a9e1-7251c608b48f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"ls /high-performance-spark-examples/warehouse/uk_gender_pay_data/metadata/\".!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb541fbf-4a79-402d-a6b2-e999106e9a18",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"cat  /high-performance-spark-examples/warehouse/uk_gender_pay_data/metadata/v1.metadata.json\".!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90149834-27a2-45a3-aa8a-dae2162da854",
   "metadata": {},
   "outputs": [],
   "source": [
    "// Iceberg Java SDK time imports\n",
    "import java.util.HashMap\n",
    "import java.util.Map\n",
    "\n",
    "import org.apache.iceberg.Table\n",
    "import org.apache.iceberg.catalog.TableIdentifier\n",
    "import org.apache.iceberg.hadoop.HadoopCatalog\n",
    "\n",
    "\n",
    "// And to handle java types\n",
    "import scala.jdk.CollectionConverters._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebf56bc6-d420-474c-b3a8-ded03b23eff8",
   "metadata": {},
   "outputs": [],
   "source": [
    "// Create a local Iceberg Catalog client. Here we're using the \"hadoop catalog\"\n",
    "// The spark hadoop conf can be got from: spark.sparkContext.hadoopConfiguration\n",
    "// Here we make the Catalog, it's kind of funky. Spark also has methods which return tables but they're Spark tables so\n",
    "// which aren't the type we want\n",
    "// https://iceberg.apache.org/javadoc/latest/org/apache/iceberg/hadoop/HadoopCatalog.html\n",
    "val catalog = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c55dc276-035f-40d4-9a47-bd4698f2519d",
   "metadata": {},
   "outputs": [],
   "source": [
    "// Now we want to load the table. To do that we need to make a TableIdentifier of the same table we wrote to. Note it'll just be\n",
    "// the table name no need for the \"local\" prefix.\n",
    "// See https://iceberg.apache.org/javadoc/1.6.0/org/apache/iceberg/catalog/TableIdentifier.html\n",
    "val name = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ea4b1cc-bd1b-42b4-bdbe-27625b461db9",
   "metadata": {},
   "outputs": [],
   "source": [
    "// Load the table\n",
    "val table = catalog.loadTable(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd1c6add-d465-4b81-9c34-6c8f40197ab2",
   "metadata": {},
   "outputs": [],
   "source": [
    "// Now we want to get the snapshots from the table. There are a few different ways we can do this:\n",
    "// 1) Using the Iceberg Table API (see https://iceberg.apache.org/javadoc/1.6.0/org/apache/iceberg/Table.html)\n",
    "// 2) Using the Iceberg + Spark SQL special query interface https://iceberg.apache.org/javadoc/1.6.0/org/apache/iceberg/Table.html\n",
    "val snapshots = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a96986d-b3a5-49ad-aeac-a492bf3fc8e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "val snapshot = snapshots(0).snapshotId()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15c6cb85-ff64-405f-ae6a-7e3c917ac12a",
   "metadata": {},
   "outputs": [],
   "source": [
    "val altSnapshotQuery = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f93516ad-3ae9-4bb6-989f-7c127f82143c",
   "metadata": {},
   "outputs": [],
   "source": [
    "val altSnapshotId = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15e67d1b-1e9e-45a0-af94-1c9c79e03d54",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"SELECT * FROM local.uk_gender_pay_data WHERE isnull(responsibleperson) LIMIT 5\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4829752b-dc30-49db-93ae-911f1c2743c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "// And the files!\n",
    "// We can also list snapshots with the select\n",
    "spark.sql(\"SELECT * FROM local.uk_gender_pay_data.files\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f262d890-0818-410a-aec8-2986a04ae16e",
   "metadata": {},
   "outputs": [],
   "source": [
    "// Lets take a quick look and see\n",
    "spark.sql(\"SELECT * FROM local.uk_gender_pay_data WHERE isnull(responsibleperson) LIMIT 5\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7369bcc8-a738-48dc-a475-55885d4460cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"DELETE FROM local.uk_gender_pay_data WHERE isnull(responsibleperson)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73d279f3-f2a5-4ddf-a56f-d473b0c28b97",
   "metadata": {},
   "outputs": [],
   "source": [
    "// Make sure the data is gone\n",
    "spark.sql(\"SELECT * FROM local.uk_gender_pay_data WHERE isnull(responsibleperson) LIMIT 5\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b6902ef-b742-466d-b4c8-d6830ff67cf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "// Yay! ok now lets travel back in time\n",
    "// We can do this with SQL or with a read option\n",
    "// SQL: https://iceberg.apache.org/docs/nightly/spark-queries/#sql"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7e899a1-d2cd-4e25-b142-e69fb9ca6774",
   "metadata": {},
   "outputs": [],
   "source": [
    "// DF: https://iceberg.apache.org/docs/nightly/spark-queries/#dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8884a8a2-bbb7-47b1-85f6-744c60612dcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(f\"SELECT * FROM local.uk_gender_pay_data.files\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f53692b-f14a-4df7-8069-147eca8da0cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"DROP TABLE IF EXISTS local.uk_gender_pay_data_postcode\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8deb38c3-1e64-4eba-ac80-c75d5674258b",
   "metadata": {},
   "outputs": [],
   "source": [
    "// Write the data out partitioned to do this we'll want to use the SQL interface so we can use the truncate function\n",
    "// since the regular Scala API doesn't support partioning by things besides raw keys.\n",
    "https://iceberg.apache.org/docs/1.5.1/spark-ddl/#partitioned-by"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e87e6b08-5c0e-4356-a0ee-7245b7d7790b",
   "metadata": {},
   "outputs": [],
   "source": [
    "// Inspect the files again. This should look familiar ish\n",
    "spark.sql(\"SELECT * FROM local.uk_gender_pay_data_postcode.files\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71569b2e-7def-42a4-bf3e-69ee9667a41d",
   "metadata": {},
   "outputs": [],
   "source": [
    "// Add some more data, we've got 2022, 2023 , & 2024\n",
    "// Make sure to use the append mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5be0f8c7-2926-4bf6-bc9d-c02a15648e83",
   "metadata": {},
   "outputs": [],
   "source": [
    "val uncompacted_file_list = \"ls -alh ../warehouse/uk_gender_pay_data/data/\".!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb6c4b7d-f8d1-41f7-b014-c6434bbb6d48",
   "metadata": {},
   "outputs": [],
   "source": [
    "val uncompacted_metadata_file_list = \"ls -alh ../warehouse/uk_gender_pay_data/metadata/\".!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfb116e4-c66d-4027-80a3-e7de9ad62ee0",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"SELECT * FROM local.uk_gender_pay_data.files\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "586bdb3c-19f0-4a63-b87f-d181e8c44c06",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"SELECT * FROM local.uk_gender_pay_data.snapshots\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22351ea4-8cb7-43c2-b205-4554d0b15aca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.iceberg.spark.actions.SparkActions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "928f9da9-d65b-4d53-b818-82a27f8171a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "// So far the logging has been... verbose but interesting, but the next stages it's actually too much\n",
    "spark.sparkContext.setLogLevel(\"ERROR\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "807193d8-8ff5-4a9c-b6ae-510ee0bb2f84",
   "metadata": {},
   "outputs": [],
   "source": [
    "// Ok let's try and compact things down a little bit.\n",
    "// You should look at SparkActions & use the rewrite data files operation.\n",
    "// Consider specifying rewrite-all to true to force rewrites\n",
    "// https://iceberg.apache.org/javadoc/latest/org/apache/iceberg/spark/actions/SparkActions.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e4a013d-1af5-4dd8-82c1-5115905f3feb",
   "metadata": {},
   "outputs": [],
   "source": [
    "val compacted_file_list = \"ls -alh ../warehouse/uk_gender_pay_data/data/\".!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c40db89-7ce1-40ed-a111-1395e5b75a0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "// Interesting. Note it _added_ a new file but the old files are all still there. That's kind of expected/ok since if we look at the\n",
    "// files actually currently used it's just the new one\n",
    "spark.sql(\"SELECT * FROM local.uk_gender_pay_data.files\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9198c74b-87d5-42b0-9987-587095848282",
   "metadata": {},
   "outputs": [],
   "source": [
    "// Remove the old snapshots but keep the latest one.\n",
    "// This produces _so much logging_ by default that running in the NB would be slow (that's why we set the log level to error)\n",
    "// Here your going to want to use the expireSnapshots action.\n",
    "// Note: if you _just set_ retainLast it will keep all snapshots, retain last is like a safety mechanism that keeps the last K\n",
    "// snapshots. To get rid of everything except the last expire everything older than right now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be51d1ca-a105-407f-ac3c-41c0f9258891",
   "metadata": {},
   "outputs": [],
   "source": [
    "val compacted_and_expired_file_list = \"ls -alh ../warehouse/uk_gender_pay_data/data/\".!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18825715-ced8-401f-b7b3-9ea682d38757",
   "metadata": {},
   "outputs": [],
   "source": [
    "// Table is in an inconsistent state here, this is not \"good\" but YOLO\n",
    "// spark.sql(\"REFRESH local.uk_gender_pay_data\").show()\n",
    "// spark.sql(\"SELECT * FROM local.uk_gender_pay_data.files\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa8644af-5604-4147-8546-f65e749b8253",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"SELECT * FROM local.uk_gender_pay_data\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f1983f2-2fe7-4e43-a78e-40fd1c7577fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "// Remove the orphaned files\n",
    "SparkActions.get().deleteOrphanFiles(table).execute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73b3e2ca-555b-467c-a253-d96aab32e27b",
   "metadata": {},
   "outputs": [],
   "source": [
    "val cleaned_and_compacted_file_list = \"ls ../warehouse/uk_gender_pay_data/data/\".!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "921d0c02-1fb7-43ec-ac0a-b5d1c3a40c3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"SELECT * FROM local.uk_gender_pay_data.files\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b8ff0a3-8c6e-4d67-8afb-d1541c7e6dbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "// Lets go take a look at a quick side-by-side test\n",
    "//cd /high-performance-spark-examples/spark-upgrade/;./e2e_demo/scala/run_demo.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05d47a57-3bfa-484a-90ed-0231a17a7205",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Scala 2.13 (w/ Spark 3.5 & Iceberg 1.6)",
   "language": "scala",
   "name": "scala2.13"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".sc",
   "mimetype": "text/x-scala",
   "name": "scala",
   "nbconvert_exporter": "script",
   "version": "2.13.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
